{"cells":[{"cell_type":"markdown","metadata":{"id":"-Raw2ptFPaNR"},"source":["# Install Necessary Libraries for the Project"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CiOGaCTPRPw"},"outputs":[],"source":["!pip install torch datasets accelerate trl"]},{"cell_type":"markdown","metadata":{"id":"lEVO20NNMhSh"},"source":["# Load and Prepare the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOJEpPGuEoVw"},"outputs":[],"source":["import os\n","import math\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from PIL import Image\n","from collections import defaultdict\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torchvision.transforms import functional as TF\n","import random\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0A9O9lUCM83_"},"outputs":[],"source":["class AugmentationTransforms:\n","    \"\"\"Custom data augmentation for handwritten math images\"\"\"\n","\n","    @staticmethod\n","    def random_rotate(image, max_angle=5):\n","        \"\"\"Randomly rotate image by small angle\"\"\"\n","        angle = random.uniform(-max_angle, max_angle)\n","        return TF.rotate(image, angle)\n","\n","    @staticmethod\n","    def random_scale(image, scale_range=(0.9, 1.1)):\n","        \"\"\"Randomly scale image\"\"\"\n","        scale = random.uniform(scale_range[0], scale_range[1])\n","        orig_size = image.size\n","        scaled_size = (int(orig_size[0] * scale), int(orig_size[1] * scale))\n","        image = TF.resize(image, scaled_size)\n","        # Resize back to original size\n","        image = TF.resize(image, orig_size)\n","        return image\n","\n","#     @staticmethod\n","#     def add_noise(image, noise_factor=0.05):\n","#         \"\"\"Add random noise to image\"\"\"\n","#         image_tensor = TF.to_tensor(image)\n","#         noise = torch.randn_like(image_tensor) * noise_factor\n","#         noisy_image = torch.clamp(image_tensor + noise, 0, 1)\n","#         return TF.to_pil_image(noisy_image)\n","\n","#     @staticmethod\n","#     def adjust_contrast(image, factor_range=(0.8, 1.2)):\n","#         \"\"\"Randomly adjust contrast\"\"\"\n","#         factor = random.uniform(factor_range[0], factor_range[1])\n","#         return TF.adjust_contrast(image, factor)\n","\n","# Enhanced Dataset with data augmentation\n","class EnhancedHandwrittenMathDataset(Dataset):\n","    def __init__(self, image_directory, labels_file, transform=None, augment=False):\n","        self.image_paths = []\n","        self.latex_sequences = []\n","        self.transform = transform\n","        self.augment = augment\n","\n","        try:\n","            with open(labels_file, 'r', encoding='utf-8') as f:\n","                for line_num, line in enumerate(f, 1):\n","                    try:\n","                        line = line.strip()\n","                        if not line:\n","                            continue\n","\n","                        parts = line.split('\\t')\n","\n","                        # Handle cases with more than 2 parts\n","                        if len(parts) > 2:\n","                            image_filename, latex_seq = parts[0], '  '.join(parts[1:])\n","                            image_path = os.path.join(image_directory, image_filename)\n","                            if os.path.exists(image_path):\n","                                self.image_paths.append(image_path)\n","                                self.latex_sequences.append(latex_seq)\n","                            else:\n","                                print(f\"Warning: Image file not found at line {line_num}: {image_path}\")\n","\n","                        # Handle cases with exactly 2 parts\n","                        elif len(parts) == 2:\n","                            image_filename, latex_seq = parts\n","                            image_path = os.path.join(image_directory, image_filename)\n","                            if os.path.exists(image_path):\n","                                self.image_paths.append(image_path)\n","                                self.latex_sequences.append(latex_seq)\n","                            else:\n","                                print(f\"Warning: Image file not found at line {line_num}: {image_path}\")\n","\n","                        # Handle cases with insufficient parts\n","                        else:\n","                            print(f\"Warning: Skipping malformed line {line_num}: {line}\")\n","\n","                    except Exception as e:\n","                        print(f\"Error processing line {line_num}: {str(e)}\")\n","                        continue\n","\n","            if not self.image_paths:\n","                print(f\"Warning: No valid image-latex pairs found in {labels_file}\")\n","\n","        except FileNotFoundError:\n","            raise FileNotFoundError(f\"Labels file not found: {labels_file}\")\n","        except PermissionError:\n","            raise PermissionError(f\"Permission denied when accessing file: {labels_file}\")\n","        except Exception as e:\n","            raise Exception(f\"Error reading labels file: {str(e)}\")\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","\n","        try:\n","            image = Image.open(image_path).convert('L')  # Convert to grayscale\n","\n","            # Apply augmentations if enabled\n","            if self.augment:\n","                # Apply random augmentations with 25% probability\n","                if random.random() > 0.25:\n","                    image = AugmentationTransforms.random_rotate(image)\n","                if random.random() > 0.25:\n","                    image = AugmentationTransforms.random_scale(image)\n","                # if random.random() > 0.25:\n","                #     image = AugmentationTransforms.add_noise(image)\n","                # if random.random() > 0.25:\n","                #     image = AugmentationTransforms.adjust_contrast(image)\n","\n","            # Apply standard transforms\n","            if self.transform:\n","                image = self.transform(image)\n","\n","            latex_seq = self.latex_sequences[idx]\n","            return image, latex_seq\n","\n","        except Exception as e:\n","            print(f\"Error loading image {image_path}: {str(e)}\")\n","            # Return a default image and empty sequence in case of error\n","            # This prevents training from crashing if a single image has issues\n","            dummy_image = torch.zeros((1, 224, 224)) if self.transform else Image.new('L', (224, 224))\n","            return dummy_image, \"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVZXX3hVAoAw"},"outputs":[],"source":["# Improved Vocabulary Builder\n","def improved_build_vocab(labels_file, vocab_size=1000):\n","    \"\"\"\n","    Build vocabulary from labels with improved error handling\n","\n","    Args:\n","        labels_file: Path to labels file\n","        vocab_size: Maximum vocabulary size\n","\n","    Returns:\n","        dict: Vocabulary mapping tokens to indices\n","    \"\"\"\n","    # Initialize collections\n","    all_chars = defaultdict(int)\n","    special_tokens = ['<pad>', '<start>', '<end>', '<unk>']\n","\n","    try:\n","        # Read and process the labels file\n","        with open(labels_file, 'r', encoding='utf-8') as f:\n","            for line_num, line in enumerate(f, 1):\n","                try:\n","                    line = line.strip()\n","                    if not line:\n","                        continue\n","\n","                    parts = line.split('\\t')\n","                    if len(parts) > 2:\n","                        latex_seq = '  '.join(parts[1:])\n","                    elif len(parts) == 2:\n","                        latex_seq = parts[1]\n","                    else:\n","                        print(f\"Warning: Skipping malformed line {line_num}: {line}\")\n","                        continue\n","\n","                    # Count character frequencies\n","                    for char in latex_seq:\n","                        all_chars[char] += 1\n","\n","                except Exception as e:\n","                    print(f\"Error processing line {line_num}: {str(e)}\")\n","                    continue\n","\n","    except Exception as e:\n","        raise Exception(f\"Error reading labels file: {str(e)}\")\n","\n","    # Sort characters by frequency (descending)\n","    sorted_chars = sorted(all_chars.items(), key=lambda x: x[1], reverse=True)\n","\n","    # Create vocabulary with special tokens first\n","    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n","\n","    # Add remaining characters up to vocab_size\n","    current_idx = len(special_tokens)\n","    for char, _ in sorted_chars:\n","        if char not in vocab and current_idx < vocab_size:\n","            vocab[char] = current_idx\n","            current_idx += 1\n","\n","    print(f\"Built vocabulary with {len(vocab)} tokens\")\n","    return vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"415y0hiC9W4W"},"outputs":[],"source":["# Improved String-Tensor Conversion Functions\n","\n","def improved_string_to_tensor(string_list, vocab):\n","    \"\"\"\n","    Convert a list of strings to a padded tensor with improved error handling.\n","    \"\"\"\n","    if isinstance(string_list, str):\n","        string_list = [string_list]\n","\n","    all_indices = []\n","    for string in string_list:\n","        indices = [vocab['<start>']] + [vocab.get(char, vocab['<unk>']) for char in string] + [vocab['<end>']]\n","        all_indices.append(torch.tensor(indices, dtype=torch.long))\n","\n","    # Fix: Set batch_first=True\n","    padded_indices = pad_sequence(all_indices, batch_first=True, padding_value=vocab['<pad>'])\n","\n","    # Create padding mask: 1 = pad token\n","    padding_mask = padded_indices.eq(vocab['<pad>'])\n","\n","    return padded_indices, padding_mask\n","\n","def improved_tensor_to_string(tensor, vocab):\n","    \"\"\"\n","    Convert a tensor of token indices to strings with improved handling\n","\n","    Args:\n","        tensor: Tensor of shape [seq_len, batch_size] or [batch_size, seq_len]\n","        vocab: Dictionary mapping tokens to indices\n","\n","    Returns:\n","        list: List of decoded strings\n","    \"\"\"\n","    # Get index-to-token mapping\n","    idx_to_token = {idx: token for token, idx in vocab.items()}\n","\n","    # If tensor is [seq_len, batch_size], convert to [batch_size, seq_len]\n","    if tensor.dim() == 2 and tensor.shape[0] < tensor.shape[1]:\n","        tensor = tensor.transpose(0, 1)\n","\n","    # Ensure tensor is on CPU\n","    tensor = tensor.cpu()\n","\n","    batch_texts = []\n","    for sequence in tensor:\n","        tokens = []\n","        for idx in sequence:\n","            token = idx_to_token.get(idx.item(), \"\")\n","            # Break at end token\n","            if token == \"<end>\":\n","                break\n","            # Skip special tokens\n","            if token not in [\"<pad>\", \"<start>\", \"<unk>\"]:\n","                tokens.append(token)\n","\n","        text = \"\".join(tokens)\n","        batch_texts.append(text)\n","\n","    return batch_texts"]},{"cell_type":"markdown","metadata":{"id":"HYI3-SoWPt7p"},"source":["# Project Code"]},{"cell_type":"markdown","metadata":{"id":"xIeaRfRanCuG"},"source":["Step 1.) CNN or Transformer based Image -> Latex conversion"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"Nut_PzM_EoVy","executionInfo":{"status":"error","timestamp":1745795592924,"user_tz":300,"elapsed":2143,"user":{"displayName":"Anthony Brogni","userId":"04470910975254961677"}},"outputId":"759df2cb-e810-4543-eaaa-2b26d985e658"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Built vocabulary with 95 tokens\n","Vocabulary size: 95\n","Training on cuda\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/50 [Train]:   0%|          | 0/44 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-0b45eabcf916>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;31m# Train model with validation removed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m train_with_scheduling(\n\u001b[0m\u001b[1;32m    426\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-31-0b45eabcf916>\u001b[0m in \u001b[0;36mtrain_with_scheduling\u001b[0;34m(model, train_loader, criterion, vocab, learning_rate, num_epochs, checkpoint_dir)\u001b[0m\n\u001b[1;32m    316\u001b[0m         )\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seqs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Iterate over the tqdm object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimproved_string_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import traceback\n","\n","# Improved Encoder using ResNet Backbone\n","class ImprovedCNNEncoder(nn.Module):\n","    def __init__(self, output_channels=256):\n","        super(ImprovedCNNEncoder, self).__init__()\n","        # Use pretrained ResNet but adapt for grayscale images\n","        resnet = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n","\n","        # Modify first conv layer to accept grayscale input (1 channel)\n","        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","\n","        # Initialize weights with first channel of pretrained model\n","        with torch.no_grad():\n","            self.conv1.weight.data = resnet.conv1.weight.data.mean(dim=1, keepdim=True)\n","\n","        # Use remaining ResNet layers\n","        self.bn1 = resnet.bn1\n","        self.relu = resnet.relu\n","        self.maxpool = resnet.maxpool\n","        self.layer1 = resnet.layer1\n","        self.layer2 = resnet.layer2\n","        self.layer3 = resnet.layer3\n","        self.layer4 = resnet.layer4\n","\n","        # Project to the desired output dimension\n","        self.proj = nn.Conv2d(512, output_channels, kernel_size=1)\n","\n","        # Adaptive pooling to ensure consistent output size\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n","\n","    def forward(self, x):\n","        # Forward pass through ResNet layers\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        # Project to desired channels\n","        x = self.proj(x)\n","\n","        # Ensure consistent spatial dims\n","        x = self.adaptive_pool(x)\n","\n","        # Reshape for transformer input [B, C, H, W] -> [B, H*W, C]\n","        batch_size, channels, height, width = x.size()\n","        x = x.view(batch_size, channels, -1).permute(0, 2, 1)\n","\n","        return x\n","\n","# Improved Transformer with Multi-Head Attention\n","class ImprovedTransformerDecoder(nn.Module):\n","    def __init__(self, vocab_size, d_model=512, num_layers=6, nhead=8,\n","                 dim_feedforward=2048, dropout=0.1, max_seq_length=200):\n","        super(ImprovedTransformerDecoder, self).__init__()\n","        self.d_model = d_model\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_embedding = nn.Embedding(max_seq_length, d_model)\n","\n","        # Use PyTorch's TransformerDecoder with more layers\n","        decoder_layer = nn.TransformerDecoderLayer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout\n","        )\n","        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n","\n","        # Output projection\n","        self.fc_out = nn.Linear(d_model, vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, tgt, memory, tgt_mask=None, tgt_padding_mask=None):\n","        # tgt: [T, B] tensor of token indices\n","        seq_len, batch_size = tgt.size()\n","\n","        # Create position indices and clamp them to valid range\n","        positions = torch.arange(0, seq_len).unsqueeze(1).expand(seq_len, batch_size).to(tgt.device)\n","        positions = positions.clamp(0, self.pos_embedding.num_embeddings - 1)\n","\n","        # Embedding with positional encoding\n","        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model) + self.pos_embedding(positions)\n","        tgt_emb = self.dropout(tgt_emb)\n","\n","        # Create mask if not provided\n","        if tgt_mask is None:\n","          tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len, dtype=torch.bool).to(tgt.device)\n","\n","        # Forward through transformer\n","        output = self.transformer_decoder(\n","            tgt_emb, memory,\n","            tgt_mask=tgt_mask,\n","            tgt_key_padding_mask=tgt_padding_mask\n","        )\n","\n","        # Project to vocabulary\n","        output = self.fc_out(output)\n","\n","        return output\n","\n","# Combined Improved Model with Beam Search\n","class ImprovedHandwrittenMathToLatexModel(nn.Module):\n","    def __init__(self, vocab_size, d_model=512):\n","        super(ImprovedHandwrittenMathToLatexModel, self).__init__()\n","        self.encoder = ImprovedCNNEncoder(output_channels=d_model)\n","        self.decoder = ImprovedTransformerDecoder(vocab_size=vocab_size, d_model=d_model)\n","\n","    def forward(self, images, tgt_seq, tgt_padding_mask=None):\n","        # images: [B, 1, H, W]\n","        # tgt_seq: [T, B]\n","        enc_out = self.encoder(images)  # [B, N, d_model]\n","\n","        # Transpose to [N, B, d_model] for transformer\n","        enc_out = enc_out.permute(1, 0, 2)\n","\n","        # Forward through decoder\n","        output = self.decoder(tgt_seq, enc_out, tgt_padding_mask=tgt_padding_mask)\n","\n","        return output\n","\n","    def beam_search_decode(self, image, vocab, beam_size=5, max_len=100):\n","        \"\"\"\n","        Perform beam search to generate LaTeX sequence.\n","\n","        Args:\n","            image: either a [C, H, W] tensor or a [1, C, H, W] tensor\n","            vocab: dict mapping tokens → indices (must include '<start>' and '<end>')\n","            beam_size: number of beams\n","            max_len: max output length\n","        \"\"\"\n","        device = next(self.parameters()).device\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 1) Massage image into shape [1, C, H, W]\n","        # ─────────────────────────────────────────────────────────────────────────\n","        img = image\n","        if img.dim() == 3:\n","            img = img.unsqueeze(0)\n","        while img.dim() > 4 or (img.dim() == 4 and img.size(0) != 1):\n","            img = img.squeeze(0)\n","        assert img.dim() == 4 and img.size(0) == 1, f\"Unrecognized image shape: {img.shape}\"\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 2) Encode\n","        # ─────────────────────────────────────────────────────────────────────────\n","        try:\n","            enc = self.encoder(img.to(device))  # [1, N, E]\n","        except Exception:\n","            print(\"ERROR in encoder call: input shape\", img.shape)\n","            traceback.print_exc()\n","            raise\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 3) Prepare memory: [N, beam_size, E]\n","        # ─────────────────────────────────────────────────────────────────────────\n","        try:\n","            memory = enc.permute(1, 0, 2)             # [N, 1, E]\n","            memory = memory.expand(-1, beam_size, -1)  # [N, beam_size, E]\n","        except Exception:\n","            print(\"ERROR permuting/expanding memory: enc.shape =\", enc.shape)\n","            traceback.print_exc()\n","            raise\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 4) Initialize beams\n","        # ─────────────────────────────────────────────────────────────────────────\n","        start_idx = vocab['<start>']\n","        end_idx   = vocab['<end>']\n","\n","        seqs   = torch.full((1, 1), start_idx, dtype=torch.long, device=device)  # [1,1]\n","        scores = torch.zeros(1, device=device)                                  # [1]\n","\n","        # ─── FIX: replicate initial <start> across beams ──────────────────────────\n","        if seqs.size(1) == 1 and memory.size(1) > 1:\n","            seqs   = seqs.expand(-1, memory.size(1)).clone()  # [1, beam_size]\n","            scores = scores.expand(memory.size(1))             # [beam_size]\n","        # ─────────────────────────────────────────────────────────────────────────\n","\n","        finished_seqs   = []\n","        finished_scores = []\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 5) Beam search loop\n","        # ─────────────────────────────────────────────────────────────────────────\n","        for step in range(max_len):\n","            # 5a) target mask\n","            tgt_len = seqs.size(0)\n","            tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n","                tgt_len, dtype=torch.bool, device=device\n","            )\n","\n","            # 5b) decoder forward\n","            try:\n","                out = self.decoder(seqs, memory, tgt_mask=tgt_mask)  # [T, B, V]\n","            except Exception:\n","                print(f\"ERROR in decoder at step {step}: seqs={seqs.shape}, memory={memory.shape}\")\n","                traceback.print_exc()\n","                raise\n","\n","            # 5c) log-probs\n","            logits   = out[-1, :, :]                 # [B, V]\n","            log_probs= F.log_softmax(logits, dim=-1) # [B, V]\n","            V = log_probs.size(-1)\n","\n","            # 5d) top-k\n","            if step == 0:\n","                topk_scores, topk_idxs = log_probs[0].topk(beam_size, dim=-1)\n","                beam_indices = torch.zeros(beam_size, dtype=torch.long, device=device)\n","            else:\n","                expanded = scores.unsqueeze(1) + log_probs  # [B, V]\n","                topk_scores, flat_idxs = expanded.view(-1).topk(beam_size, dim=-1)\n","                beam_indices = flat_idxs // V\n","                topk_idxs    = flat_idxs % V\n","\n","            # 5e) build new beams\n","            new_seqs   = []\n","            new_scores = []\n","            for b_i, tok_i, sc in zip(beam_indices, topk_idxs, topk_scores):\n","                candidate = torch.cat([seqs[:, b_i], tok_i.unsqueeze(0)])\n","                if tok_i.item() == end_idx:\n","                    finished_seqs.append(candidate)\n","                    finished_scores.append(sc)\n","                else:\n","                    new_seqs.append(candidate.unsqueeze(1))\n","                    new_scores.append(sc)\n","\n","            # 5f) exit if no active beams\n","            if not new_seqs:\n","                break\n","\n","            # 5g) update\n","            seqs   = torch.cat(new_seqs, dim=1)  # [step+1, beam_count]\n","            scores = torch.stack(new_scores)     # [beam_count]\n","\n","            # 5h) reorder memory\n","            keep   = beam_indices[: len(new_seqs)]\n","            memory = memory.index_select(1, keep)\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 6) finalize: if none finished, treat current beams as finished\n","        # ─────────────────────────────────────────────────────────────────────────\n","        if not finished_seqs:\n","            for b in range(seqs.size(1)):\n","                seq = torch.cat([seqs[:, b], torch.tensor([end_idx], device=device)])\n","                finished_seqs.append(seq)\n","                finished_scores.append(scores[b])\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 7) choose best\n","        # ─────────────────────────────────────────────────────────────────────────\n","        best_idx = torch.tensor(finished_scores).argmax().item()\n","        return finished_seqs[best_idx]\n","\n","# Training with Early Stopping and Learning Rate Scheduling\n","class EarlyStopping:\n","    \"\"\"Early stopping to prevent overfitting\"\"\"\n","    def __init__(self, patience=5, min_delta=0, verbose=True):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.verbose = verbose\n","\n","    def __call__(self, val_loss):\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            return False\n","\n","        if score < self.best_score + self.min_delta:\n","            self.counter += 1\n","            if self.verbose:\n","                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","                return True\n","        else:\n","            self.best_score = score\n","            self.counter = 0\n","            return False\n","\n","def train_with_scheduling(model, train_loader, criterion, vocab,\n","                          learning_rate=1e-4, num_epochs=5,\n","                          checkpoint_dir='checkpoints'):\n","    \"\"\"\n","    Train model with learning rate scheduling and tqdm progress bar.\n","    Validation phase removed for faster training.\n","    \"\"\"\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Training on {device}\")\n","\n","    model = model.to(device)\n","    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","    pad_idx = vocab['<pad>']\n","\n","    for epoch in range(num_epochs):\n","        # --- Training phase ---\n","        model.train()\n","        train_loss = 0.0\n","        num_train_batches = len(train_loader)\n","\n","        # Create tqdm progress bar for training loop\n","        # Wrap enumerate(train_loader) and provide total for accurate progress\n","        pbar_train = tqdm(\n","            enumerate(train_loader),\n","            total=num_train_batches,\n","            desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"\n","        )\n","\n","        for batch_idx, (images, target_seqs) in pbar_train: # Iterate over the tqdm object\n","            images = images.to(device)\n","            target_tensor, _ = improved_string_to_tensor(target_seqs, vocab)\n","            target_tensor = target_tensor.to(device)\n","\n","            input_tensor_bfirst = target_tensor[:, :-1]\n","            target_labels_bfirst = target_tensor[:, 1:]\n","            input_tensor = input_tensor_bfirst.transpose(0, 1)\n","            input_padding_mask = (input_tensor_bfirst == pad_idx).to(device)\n","            target_labels = target_labels_bfirst.transpose(0, 1)\n","\n","            # --- Forward pass ---\n","            optimizer.zero_grad()\n","            output = model(images, input_tensor, tgt_padding_mask=input_padding_mask)\n","\n","            # --- Compute loss ---\n","            loss = criterion(output.reshape(-1, output.shape[-1]), target_labels.reshape(-1))\n","\n","            # --- Backward pass ---\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","\n","            current_loss = loss.item()\n","            train_loss += current_loss\n","\n","            # Update progress bar postfix with current batch loss\n","            pbar_train.set_postfix({'loss': f'{current_loss:.4f}'})\n","\n","        # Calculate average training loss for the epoch\n","        avg_train_loss = train_loss / num_train_batches\n","        # Optional: Update the postfix one last time to show average loss for the epoch\n","        pbar_train.set_postfix({'avg_loss': f'{avg_train_loss:.4f}'})\n","        pbar_train.close() # Close the training progress bar\n","\n","        # --- Save checkpoint every nth epoch\n","        n = 10\n","        print(f'Epoch {epoch+1}/{num_epochs} -> Avg Train Loss: {avg_train_loss:.4f}')\n","        if (epoch + 1) % n == 0:\n","          checkpoint_path = os.path.join(\n","              checkpoint_dir,\n","              f'model_epoch_{epoch+1}_train_loss_{avg_train_loss:.4f}.pt'\n","          )\n","          torch.save({\n","              'epoch': epoch + 1,\n","              'model_state_dict': model.state_dict(),\n","              'optimizer_state_dict': optimizer.state_dict(),\n","              'train_loss': avg_train_loss,\n","              'vocab': vocab,\n","          }, checkpoint_path)\n","          print(f'Saved model checkpoint to {checkpoint_path}')\n","\n","    print(\"Training finished.\")\n","\n","# Paths\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","folder_path = '/content/drive/My Drive/Senior Year/Spring Semester/CSCI 5527/CSCI 5527 Project/Data/3312_images'\n","image_directory = os.path.join(folder_path, \"synthetic_images\")\n","labels_file = os.path.join(folder_path, \"synthetic_labels.txt\")\n","checkpoint_dir = 'improved_checkpoints'\n","\n","# Create vocabulary\n","vocab = improved_build_vocab(labels_file, vocab_size=1000)\n","vocab_size = len(vocab)\n","print(f\"Vocabulary size: {vocab_size}\")\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resize for ResNet\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale images\n","])\n","\n","# Create datasets with augmentation for training\n","full_dataset = EnhancedHandwrittenMathDataset(\n","    image_directory=image_directory,\n","    labels_file=labels_file,\n","    transform=transform,\n","    augment=True  # Enable augmentation for training\n",")\n","\n","# Split dataset - we'll just use train and test since we're removing validation\n","train_size = int(0.9 * len(full_dataset))\n","test_size = len(full_dataset) - train_size\n","\n","train_dataset, test_dataset = random_split(\n","    full_dataset,\n","    [train_size, test_size],\n","    generator=torch.Generator().manual_seed(42)\n",")\n","\n","# Disable augmentation for test dataset\n","test_dataset.dataset.augment = False\n","\n","# Create dataloaders\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=1)\n","\n","# Create model\n","model = ImprovedHandwrittenMathToLatexModel(vocab_size=vocab_size)\n","\n","# Define loss function that ignores padding\n","criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n","\n","# Train model with validation removed\n","train_with_scheduling(\n","    model=model,\n","    train_loader=train_loader,\n","    criterion=criterion,\n","    vocab=vocab,\n","    learning_rate=1e-4,\n","    num_epochs=50,\n","    checkpoint_dir=checkpoint_dir,\n",")"]},{"cell_type":"markdown","metadata":{"id":"lJPbIDhILlK4"},"source":["**Step 2.) Finetune an LLM using GRPO training to correct errors in the Latex syntax**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oqI6OLsBEoVy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745795813269,"user_tz":300,"elapsed":208646,"user":{"displayName":"Anthony Brogni","userId":"04470910975254961677"}},"outputId":"a64a9afd-e794-4c70-b199-8175e0adc0d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating on device: cuda\n","Loaded checkpoint: epoch 50  train_loss=0.2609\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating [Test]: 100%|██████████| 5/5 [00:01<00:00,  4.34it/s, batch_loss=2.7572, acc=50.60%]\n","Decoding & Saving: 100%|██████████| 5/5 [03:26<00:00, 41.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Saved predictions to test_predictions.csv\n","                                          prediction  \\\n","0                                          X _ 1 , 2   \n","1  \\ m a t h b f { V } _ { n }   =   \\ s u m _ { ...   \n","2  \\ o p e r a t o r n a m e { H } _ N   ( x ,   ...   \n","3  \\ v e c { B } _ 0   =   \\ f r a c { \\ p a r t ...   \n","4                                          v _ ( x )   \n","\n","                                           reference  \n","0                                           X_i, Y_i  \n","1  m = \\int_{x=0}^2 \\int_{y=x}^{4-x} \\,(2x+3y+2)\\...  \n","2  \\forall x'\\in X: d_Y(f(x),f(x'))\\leq\\omega(d_X...  \n","3  dJ_z/dt = {\\partial J_z\\over \\partial t} + {\\p...  \n","4                                            v_2 (x)  \n","\n","Test set → Avg per-token loss: 2.6992 | Token accuracy: 52.00%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import os\n","import torch\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import pandas as pd\n","\n","# ─────────────────────────────────────────────────────────────────────────────\n","# 1. Configuration\n","# ─────────────────────────────────────────────────────────────────────────────\n","checkpoint_dir  = 'improved_checkpoints'\n","checkpoint_name = 'model_epoch_50_train_loss_0.2609.pt'  # adjust as needed\n","checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n","\n","# Assume test_loader, vocab, and improved_string_to_tensor are already defined\n","pad_idx = vocab['<pad>']\n","\n","# ─────────────────────────────────────────────────────────────────────────────\n","# 2. Model & checkpoint loading\n","# ─────────────────────────────────────────────────────────────────────────────\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Evaluating on device: {device}\")\n","\n","model = ImprovedHandwrittenMathToLatexModel(vocab_size=len(vocab), d_model=512)\n","model = model.to(device)\n","\n","ckpt = torch.load(checkpoint_path, map_location=device)\n","model.load_state_dict(ckpt['model_state_dict'])\n","model.eval()\n","print(f\"Loaded checkpoint: epoch {ckpt['epoch']}  train_loss={ckpt['train_loss']:.4f}\")\n","\n","# ─────────────────────────────────────────────────────────────────────────────\n","# 3. Evaluation with tqdm\n","# ─────────────────────────────────────────────────────────────────────────────\n","total_loss      = 0.0\n","total_tokens    = 0\n","correct_tokens  = 0\n","criterion       = torch.nn.CrossEntropyLoss(ignore_index=pad_idx, reduction='sum')\n","\n","with torch.no_grad():\n","    pbar = tqdm(test_loader, desc=\"Evaluating [Test]\")\n","    for images, target_seqs in pbar:\n","        # Prepare tensors\n","        images = images.to(device)\n","        target_tensor, _ = improved_string_to_tensor(target_seqs, vocab)\n","        target_tensor = target_tensor.to(device)\n","\n","        input_tensor    = target_tensor[:, :-1].transpose(0,1)\n","        labels          = target_tensor[:,  1:].transpose(0,1)\n","        padding_mask    = (target_tensor[:, :-1] == pad_idx).to(device)\n","\n","        # Forward\n","        outputs = model(images, input_tensor, tgt_padding_mask=padding_mask)\n","        # outputs: [T-1, B, V]\n","\n","        # Loss\n","        logits_flat = outputs.reshape(-1, outputs.size(-1))\n","        labels_flat = labels.reshape(-1)\n","        loss        = criterion(logits_flat, labels_flat)\n","        total_loss += loss.item()\n","\n","        # Token‐level accuracy\n","        preds        = logits_flat.argmax(dim=-1)\n","        mask         = labels_flat != pad_idx\n","        correct      = (preds[mask] == labels_flat[mask]).sum().item()\n","        total_tokens += mask.sum().item()\n","        correct_tokens += correct\n","\n","        # Update tqdm postfix\n","        pbar.set_postfix({\n","            'batch_loss': f\"{(loss.item()/mask.sum().item()):.4f}\",\n","            'acc':        f\"{(correct/mask.sum().item()*100):.2f}%\"\n","        })\n","\n","# Build reverse vocab\n","idx_to_token = {idx: tok for tok, idx in vocab.items()}\n","\n","def tensor_to_string(tensor_seq, idx_to_token):\n","    \"\"\"Convert a 1D LongTensor of token IDs (including <start> and <end>) to a string.\"\"\"\n","    tokens = []\n","    for idx in tensor_seq.cpu().tolist():\n","        tok = idx_to_token.get(idx, '<unk>')\n","        if tok in ['<start>', '<end>', '<pad>']:\n","            continue\n","        tokens.append(tok)\n","    return ' '.join(tokens)\n","\n","# Lists to collect\n","decoded_list = []\n","reference_list = []\n","\n","model.eval()\n","with torch.no_grad():\n","    pbar = tqdm(test_loader, desc=\"Decoding & Saving\")\n","    for images, target_seqs in pbar:\n","        images = images.to(device)  # [B,1,H,W]\n","        for img_tensor, ref_str in zip(images, target_seqs):\n","            # img_tensor arrives as [1, C, H, W] or maybe [C, H, W]\n","            img = img_tensor\n","\n","            # 1) If it has a leading batch dim of 1 (shape [1, C, H, W]), remove it:\n","            if img.dim() == 4 and img.size(0) == 1:\n","                img = img.squeeze(0)\n","\n","            # 2) If it somehow has two batch dims ([1, 1, C, H, W]), remove both:\n","            while img.dim() > 3:\n","                img = img.squeeze(0)\n","\n","            # Now img.dim() should be exactly 3: [C, H, W]\n","            assert img.dim() == 3, f\"Expected 3D image, got {img.shape}\"\n","\n","            # Move to device\n","            img = img.to(device)\n","\n","            # Decode\n","            pred_seq = model.beam_search_decode(img, vocab, beam_size=5, max_len=100)\n","\n","            # Convert to string\n","            decoded_str = tensor_to_string(pred_seq, idx_to_token)\n","            decoded_list.append(decoded_str)\n","            reference_list.append(ref_str)\n","\n","# Save to CSV\n","df = pd.DataFrame({'prediction': decoded_list, 'reference': reference_list})\n","csv_path = 'test_predictions.csv'\n","df.to_csv(csv_path, index=False)\n","print(f\"Saved predictions to {csv_path}\")\n","print(df.head())\n","\n","# ─────────────────────────────────────────────────────────────────────────────\n","# 4. Final metrics\n","# ─────────────────────────────────────────────────────────────────────────────\n","avg_loss = total_loss / total_tokens\n","tok_acc  = correct_tokens / total_tokens * 100\n","\n","print(f\"\\nTest set → Avg per-token loss: {avg_loss:.4f} | Token accuracy: {tok_acc:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KAU-iLliEoVz"},"outputs":[],"source":["import pandas as pd\n","size = len(list(dataset))\n","df = pd.DataFrame()\n","rows_to_append = []\n","df_columns = pd.DataFrame(columns=['label'])\n","for i in range(size):\n","    rows_to_append.append({'label': dataset.__getitem__(i)[1]})\n","df = pd.concat([df, pd.DataFrame(rows_to_append)], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hjuC982EoVz"},"outputs":[],"source":["%%writefile train_grpo.py\n","\n","import os\n","import torch\n","import numpy as np\n","import pandas as pd\n","from trl import GRPOConfig, GRPOTrainer\n","from rapidfuzz import fuzz\n","from datasets import Dataset\n","from datetime import datetime\n","\n","# Create the prompts for GRPO Training\n","def create_prompt(example):\n","    example[\"prompt\"] = f\"\"\"Please ensure that the following text is valid LaTeX by fixing syntax issues as needed. Here is the potentially invalid LaTeX: {example[\"Model Output\"]}. What is the fixed valid LaTeX: \"\"\"\n","    return example\n","\n","# Read in the dataset to use from csv file\n","df = pd.read_csv(\"model_eval_resul.csv\")\n","df = df.apply(create_prompt, axis=1)\n","dataset2 = Dataset.from_pandas(df)\n","print(dataset2)\n","\n","# Determine device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Training will run on: {device}\")\n","\n","# Create a unique checkpoint directory for each run using a timestamp\n","run = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","checkpoint_dir = f'/home/csci5527/shared/the_gradients/5527/{run}'\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","def reward(completions, **kwargs):\n","    \"\"\"Reward function that rewards a similarity score between two strings in the range [0,1].\"\"\"\n","    correct_latex = kwargs[\"Expected Label\"]\n","    rewards = []\n","    for completion, reference in zip(completions, correct_latex):\n","      if not completion or not reference:\n","        rewards.append(0.0)\n","        continue\n","      # Do not reward empty strings\n","      if len(completion) == 0:\n","            rewards.append(0.0)\n","            continue\n","      # Perfect match gets a full reward\n","      if completion == reference:\n","          rewards.append(1.0)\n","          continue\n","      # Apply RapidFuzz ratio for all cases (handles different lengths well)\n","      similarity = fuzz.ratio(completion, reference) / 100.0\n","      # Add additional penalty for length mismatch\n","      length_penalty = max(0, 1 - (abs(len(completion) - len(reference)) / max(len(reference), 1)))\n","      # Combined score is a linear combination of similarity and length_penalty\n","      final_score = (similarity * 0.5) + (length_penalty * 0.5)\n","      rewards.append(final_score)\n","    return rewards\n","\n","training_args = GRPOConfig(\n","    output_dir=checkpoint_dir,\n","    logging_steps=50,\n","    per_device_train_batch_size=4,  # Decrease this to lower vram usage\n","    num_generations=4,  # Decrease this to lower vram usage\n","    save_strategy=\"no\",  # Do not save checkpoints (saves storage space)\n","    bf16=True,  # Enable bf16 mixed precision on A100 GPUs\n",")\n","\n","trainer = GRPOTrainer(\n","    model=\"microsoft/Phi-4-mini-instruct\",\n","    reward_funcs=reward,\n","    args=training_args,\n","    train_dataset=dataset2,\n",")\n","\n","# Train and save the final model\n","trainer.train()\n","trainer.save_model(os.path.join(checkpoint_dir, \"final_model\"))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["-Raw2ptFPaNR"],"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"main","language":"python","name":"main"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"}},"nbformat":4,"nbformat_minor":0}