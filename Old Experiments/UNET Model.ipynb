{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyNsmRrsxo0Wf8KTtyNY8y3F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"afkWmIqr27-e","executionInfo":{"status":"ok","timestamp":1743966341096,"user_tz":300,"elapsed":4181,"user":{"displayName":"Ajitesh Parthasarathy","userId":"10718185110510952821"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","\n","class UNET(torch.nn.Module):\n","    def __init__(self,\n","               input_shape: int,\n","                 vocab_size: int) -> None:\n","        super(UNET, self).__init__()\n","\n","        # Encoder\n","\n","\n","        self.encoder1 = nn.Sequential(\n","            nn.Conv2d(input_shape,64,kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64,kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.pool1 = nn.MaxPool2d(2)\n","\n","        self.encoder2 = nn.Sequential(\n","            nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128,128,kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.pool2 = nn.MaxPool2d(2)\n","\n","        self.encoder3 = nn.Sequential(\n","            nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256,kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.pool3 = nn.MaxPool2d(2)\n","\n","        self.encoder4 = nn.Sequential(\n","            nn.Conv2d(256,512,kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512,512,kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.pool4 = nn.MaxPool2d(2)\n","\n","        self.encoder5 = nn.Sequential(\n","            nn.Conv2d(512,1024,kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(1024,1024,kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","\n","\n","        # Decoder\n","        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2,stride=2)\n","        self.decoder1 = nn.Sequential(\n","            nn.Conv2d(1024, 512, kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512,512,kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2,stride=2)\n","        self.decoder2 = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256,kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","\n","        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2,stride=2)\n","        self.decoder3 = nn.Sequential(\n","            nn.Conv2d(256, 128,kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 128, kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.upconv4 = nn.ConvTranspose2d(128,64, kernel_size=2,stride=2)\n","        self.decoder4 = nn.Sequential(\n","            nn.Conv2d(128,64, kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64,64, kernel_size=3,stride=1,padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        #self.classifier = nn.Sequential(nn.Conv2d(64, output_shape, 3, 1, padding=1),nn.ReLU())\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((20, 1))\n","        self.lstm = nn.LSTM(64, 512, num_layers=2, batch_first=True)\n","        self.fc = nn.Linear(512, vocab_size)\n","        self.dropout = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        # Encoder\n","        x1 = self.encoder1(x)\n","        x = self.pool1(x1)\n","        #print(\"x1\",x.shape)\n","        x2 = self.encoder2(x)\n","        x = self.pool2(x2)\n","        #print(\"x2\",x.shape)\n","        x3 = self.encoder3(x)\n","\n","        x = self.pool3(x3)\n","        #print(\"x3\",x.shape)\n","        x4 = self.encoder4(x)\n","\n","        x = self.pool4(x4)\n","\n","        x = self.encoder5(x)\n","\n","\n","        # Decoder\n","        x = self.upconv1(x)\n","\n","        x = torch.cat([x, x4], dim=1) # Skip connection\n","        x = self.decoder1(x)\n","        #print(\"x1\",x.shape)\n","        x = self.upconv2(x)\n","        x = torch.cat([x, x3], dim=1) # Skip connection\n","        x = self.decoder2(x)\n","        #print(\"x2\",x.shape)\n","        x = self.upconv3(x)\n","        x = torch.cat([x, x2], dim=1) # Skip connection\n","        x = self.decoder3(x)\n","        #print(\"x3\",x.shape)\n","        x = self.upconv4(x)\n","        x = torch.cat([x, x1], dim=1) # Skip connection\n","        x = self.decoder4(x)\n","        #print(\"x4\",x.shape)\n","\n","        x = self.adaptive_pool(x)\n","        x = x.squeeze(-1)\n","        x = x.permute(0, 2, 1)\n","        #batch_size = x.size(0)\n","        #x = x.view(batch_size, 1024, -1).permute(0, 2, 1)  # [batch, seq, features]\n","        lstm_out, _ = self.lstm(x)\n","        outputs = self.fc(self.dropout(lstm_out))\n","\n","        #x = self.classifier(x)\n","        #print(x.shape)\n","        #x = self.dropout(x)\n","        return outputs"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","folder_path = '/content/drive/MyDrive/5527Pdata/'\n","import os\n","print(os.listdir(folder_path))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QLsrDM12DL0q","executionInfo":{"status":"ok","timestamp":1743966359318,"user_tz":300,"elapsed":18207,"user":{"displayName":"Ajitesh Parthasarathy","userId":"10718185110510952821"}},"outputId":"8cf83db5-c0ae-4065-fa08-fcdafda4671a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","['synthetic_images', 'synthetic_labels.txt', 'test_images', 'test_labels.txt', 'train_images', 'train_labels.txt', 'valid_images', 'valid_labels.txt', 'symbols_images', 'symbols_labels.txt']\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms\n","from sklearn.model_selection import train_test_split\n","\n","\n","class ImageTextDataset(Dataset):\n","    def __init__(self, label_file_path, image_dir, transform=None, vocab=None, max_length=20):\n","        self.image_dir = image_dir\n","        self.transform = transform or transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor()\n","        ])\n","        self.vocab = vocab\n","        self.max_length = max_length\n","\n","        # Read labels and image names\n","        self.image_names = []\n","        self.raw_labels = []  # Store raw text labels\n","        with open(label_file_path, 'r') as f:\n","            for line in f:\n","                if line.strip():\n","                    parts = line.strip().split('\\t')\n","                    if len(parts) == 2:\n","                        self.image_names.append(parts[0])\n","                        self.raw_labels.append(parts[1])\n","\n","        # Only process to tokens if vocab exists\n","        self.labels = []\n","        if self.vocab is not None:\n","            for label in self.raw_labels:\n","                tokens = label.split('\\\\')[:self.max_length]\n","                encoded = [self.vocab.get(token, 1) for token in tokens]  # 1 = <unk>\n","                padded = encoded + [0]*(self.max_length - len(encoded))\n","                self.labels.append(torch.tensor(padded))\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        # Load image\n","        img_path = os.path.join(self.image_dir, self.image_names[idx])\n","        image = Image.open(img_path).convert('RGB')\n","        image = self.transform(image)\n","\n","        # Return encoded label if vocab exists, else raw text\n","        if self.vocab is not None:\n","            return image, self.labels[idx]\n","        else:\n","            return image, self.raw_labels[idx]\n","\n","\n","\n","def create_train_test_split(dataset, test_size=0.2, random_state=42):\n","    \"\"\"\n","    Split the dataset into training and testing sets\n","\n","    Args:\n","        dataset (ImageTextDataset): The dataset to split\n","        test_size (float): Proportion of the dataset to include in the test split\n","        random_state (int): Random seed for reproducibility\n","\n","    Returns:\n","        tuple: (train_dataset, test_dataset)\n","    \"\"\"\n","    # Method 1: Using PyTorch's random_split\n","    train_size = int((1 - test_size) * len(dataset))\n","    test_size = len(dataset) - train_size\n","\n","    train_dataset, test_dataset = random_split(\n","        dataset,\n","        [train_size, test_size],\n","        generator=torch.Generator().manual_seed(random_state)\n","    )\n","\n","    return train_dataset, test_dataset\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","\n","\n","    # Example paths\n","    label_file_path = folder_path + \"synthetic_labels.txt\"\n","    image_dir = folder_path + \"synthetic_images\"\n","\n","    # Define transformations\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),  # Resize images for consistency\n","        transforms.ToTensor(),           # Convert PIL images to tensors\n","    ])\n","\n","    # Create dataset with transform\n","    dataset = ImageTextDataset(label_file_path, image_dir, transform=transform)\n","\n","    # Split dataset\n","    train_dataset, test_dataset = create_train_test_split(dataset, test_size=0.2)\n","\n","    # Create dataloaders\n","    batch_size = 4\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","    # Print dataset statistics\n","    print(f\"Full dataset size: {len(dataset)}\")\n","    print(f\"Training set size: {len(train_dataset)}\")\n","    print(f\"Test set size: {len(test_dataset)}\")\n","\n","    # Try accessing a batch from the training dataloader\n","    print(\"\\nExample training batch:\")\n","    for images, labels in train_loader:\n","        print(f\"Image batch shape: {images.shape}\")\n","        print(f\"Labels: {labels}\")\n","        break  # Only print first batch\n","\n","    # Try accessing a batch from the test dataloader\n","    print(\"\\nExample test batch:\")\n","    for images, labels in test_loader:\n","        print(f\"Image batch shape: {images.shape}\")\n","        print(f\"Labels: {labels}\")\n","        break  # Only print first batch\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MYCWrqn-C1ft","executionInfo":{"status":"ok","timestamp":1743966369352,"user_tz":300,"elapsed":10009,"user":{"displayName":"Ajitesh Parthasarathy","userId":"10718185110510952821"}},"outputId":"bfba1fdc-f964-4442-a811-f875592ca55c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Full dataset size: 100\n","Training set size: 80\n","Test set size: 20\n","\n","Example training batch:\n","Image batch shape: torch.Size([4, 3, 224, 224])\n","Labels: ('d_{FG}=d_F+d_G.', '\\\\varphi(x \\\\alpha, y \\\\beta) = \\\\sigma(\\\\alpha) \\\\, \\\\varphi(x, y) \\\\, \\\\beta .', 'B = \\\\operatorname{core}B.', '\\\\hat{\\\\mathbf x} = R_1^{-1} \\\\left(Q_1^\\\\textsf{T} \\\\mathbf{b}\\\\right)')\n","\n","Example test batch:\n","Image batch shape: torch.Size([4, 3, 224, 224])\n","Labels: ('Az+B', '\\\\int (\\\\sin(x)+1)dx=\\\\int \\\\sin(x)dx + \\\\int 1dx=-\\\\cos(x)+x+C', 'm^*\\\\left(E, \\\\hat{B}, k_{\\\\hat{B}}\\\\right) = \\\\frac{\\\\hbar^2}{2\\\\pi} \\\\cdot \\\\frac{\\\\partial}{\\\\partial E} A\\\\left(E, \\\\hat{B}, k_{\\\\hat{B}}\\\\right)', '\\\\tan(-\\\\theta) = -\\\\tan \\\\theta')\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import os\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms\n","from collections import Counter\n","def build_vocabulary(dataset):\n","    # Define special tokens first\n","    special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n","    vocabulary = {token: idx for idx, token in enumerate(special_tokens)}\n","\n","    # Collect all tokens from dataset\n","    all_tokens = []\n","    for _, label in dataset:\n","        all_tokens.extend(label.split('\\\\'))\n","\n","    # Create word counts while filtering out special tokens\n","    word_counts = Counter(\n","        token for token in all_tokens\n","        if token not in special_tokens\n","    )\n","\n","    # Add regular tokens after special tokens\n","    for idx, (token, _) in enumerate(word_counts.most_common()):\n","        vocabulary[token] = idx + len(special_tokens)\n","\n","    return vocabulary\n","\n","temp_dataset = ImageTextDataset(label_file_path, image_dir)\n","vocab = build_vocabulary(temp_dataset)\n","vocab_size = len(vocab)"],"metadata":{"id":"VMv9hCTQJZdm","executionInfo":{"status":"ok","timestamp":1743967434344,"user_tz":300,"elapsed":414,"user":{"displayName":"Ajitesh Parthasarathy","userId":"10718185110510952821"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["import torch\n","from collections import Counter\n","\n","\n","for images, labels in test_loader:\n","  # Sample text\n","  text = labels[0]\n","\n","  # 1. Tokenization (split by space)\n","  tokens = text.split('\\\\')\n","\n","  # 2. Vocabulary Creation\n","  word_counts = Counter(tokens)\n","  vocabulary = {token: index for index, token in enumerate(word_counts)}\n","\n","  # 3. Numerical Encoding\n","  encoded_text = [vocabulary[token] for token in tokens]\n","\n","  # 4. Tensor Conversion\n","  text_tensor = torch.tensor(encoded_text)\n","\n","  print(\"Original Text:\", text)\n","  print(\"Tokens:\", tokens)\n","  print(\"Vocabulary:\", vocabulary)\n","  print(\"Vocabulary size :\", len(vocabulary))\n","  print(\"Encoded Text:\", encoded_text)\n","  print(\"Tensor:\", text_tensor)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hE4zDKCNMCjd","executionInfo":{"status":"ok","timestamp":1743966400121,"user_tz":300,"elapsed":9,"user":{"displayName":"Ajitesh Parthasarathy","userId":"10718185110510952821"}},"outputId":"23a4304f-b784-4c4f-c028-c14c4fafac7e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Text: Az+B\n","Tokens: ['Az+B']\n","Vocabulary: {'Az+B': 0}\n","Vocabulary size : 1\n","Encoded Text: [0]\n","Tensor: tensor([0])\n"]}]},{"cell_type":"code","source":["# Full dataset with transforms\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","dataset = ImageTextDataset(label_file_path, image_dir, transform=transform, vocab=vocab)\n","\n","# Split dataset\n","train_size = int(0.8 * len(dataset))\n","test_size = len(dataset) - train_size\n","train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","# Data loaders\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=1)"],"metadata":{"id":"OOGjwgNodIeg","executionInfo":{"status":"ok","timestamp":1743967585064,"user_tz":300,"elapsed":4,"user":{"displayName":"Ajitesh Parthasarathy","userId":"10718185110510952821"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","from torchvision import models\n","class CNNTransformer(nn.Module):\n","    def __init__(self, vocab_size, max_seq_length=20):\n","        super().__init__()\n","        # Image encoder\n","        self.cnn = models.resnet50(pretrained=True)\n","        self.cnn.fc = nn.Sequential(\n","            nn.Linear(self.cnn.fc.in_features, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3)\n","        )\n","\n","        # Text decoder\n","        self.embedding = nn.Embedding(vocab_size, 512)\n","        self.transformer_decoder = nn.TransformerDecoder(\n","            nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True),  # Add batch_first\n","            num_layers=3\n","        )\n","        self.fc = nn.Linear(512, vocab_size)\n","        self.max_seq_length = max_seq_length\n","\n","        # Positional encoding\n","        self.positional_encoding = nn.Parameter(\n","            torch.zeros(1, max_seq_length, 512)\n","        )\n","        nn.init.trunc_normal_(self.positional_encoding)\n","\n","    def forward(self, images, captions=None):\n","        # Encode images\n","        img_features = self.cnn(images)  # [batch, 512]\n","        img_features = img_features.unsqueeze(1)  # [batch, 1, 512]\n","\n","        if captions is None:\n","            return self.generate(img_features)\n","\n","        # Embed captions\n","        seq_length = captions.size(1)\n","        embeddings = self.embedding(captions)  # [batch, seq_len, 512]\n","        embeddings += self.positional_encoding[:, :seq_length, :]\n","\n","        # Transformer decoding\n","        decoder_output = self.transformer_decoder(\n","            tgt=embeddings,\n","            memory=img_features\n","        )\n","\n","        return self.fc(decoder_output)\n","\n","    def generate(self, img_features, temperature=1.0):\n","        batch_size = img_features.size(0)\n","        device = img_features.device\n","\n","        # Start with <sos> token\n","        outputs = torch.full((batch_size, 1), 2, dtype=torch.long, device=device)\n","\n","        for _ in range(self.max_seq_length-1):\n","            embeddings = self.embedding(outputs) + self.positional_encoding[:, :outputs.size(1), :]\n","            print(img_features.shape)\n","\n","            decoder_out = self.transformer_decoder(\n","                tgt=embeddings,\n","                memory=img_features\n","            )\n","\n","            logits = self.fc(decoder_out[:, -1, :]) / temperature\n","            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n","            outputs = torch.cat([outputs, next_token], dim=1)\n","\n","        return outputs"],"metadata":{"id":"HeOc76OJgsNq","executionInfo":{"status":"ok","timestamp":1743967634716,"user_tz":300,"elapsed":51,"user":{"displayName":"Ajitesh Parthasarathy","userId":"10718185110510952821"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","from torchvision import models\n","class CNNTransformer(nn.Module):\n","    def __init__(self, vocab_size, max_seq_length=20):\n","        super().__init__()\n","        # Image encoder\n","        self.cnn = models.resnet50(pretrained=True)\n","        self.cnn.fc = nn.Sequential(\n","            nn.Linear(self.cnn.fc.in_features, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3)\n","        )\n","\n","        # Text decoder\n","        self.embedding = nn.Embedding(vocab_size, 512)\n","        self.transformer_decoder = nn.TransformerDecoder(\n","            nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True),  # Add batch_first\n","            num_layers=3\n","        )\n","        self.fc = nn.Linear(512, vocab_size)\n","        self.max_seq_length = max_seq_length\n","\n","        # Positional encoding\n","        self.positional_encoding = nn.Parameter(\n","            torch.zeros(1, max_seq_length, 512)\n","        )\n","        nn.init.trunc_normal_(self.positional_encoding)\n","\n","    def forward(self, images, captions=None):\n","        # Encode images\n","        img_features = self.cnn(images)  # [batch, 512]\n","        img_features = img_features.unsqueeze(1)  # [batch, 1, 512]\n","\n","        if captions is None:\n","            return self.generate(img_features)\n","\n","        # Embed captions\n","        seq_length = captions.size(1)\n","        embeddings = self.embedding(captions)  # [batch, seq_len, 512]\n","        embeddings += self.positional_encoding[:, :seq_length, :]\n","\n","        # Transformer decoding\n","        decoder_output = self.transformer_decoder(\n","            tgt=embeddings,\n","            memory=img_features\n","        )\n","\n","        return self.fc(decoder_output)\n","\n","    def generate(self, img_features, temperature=1.0):\n","        with torch.no_grad():\n","          img_features = self.cnn(images).unsqueeze(1)\n","        batch_size = img_features.size(0)\n","        device = img_features.device\n","\n","        # Start with <sos> token\n","        outputs = torch.full((batch_size, 1), 2, dtype=torch.long, device=device)\n","\n","        for _ in range(self.max_seq_length-1):\n","            embeddings = self.embedding(outputs) + self.positional_encoding[:, :outputs.size(1), :]\n","            print(img_features.shape)\n","\n","            decoder_out = self.transformer_decoder(\n","                tgt=embeddings,\n","                memory=img_features\n","            )\n","\n","            logits = self.fc(decoder_out[:, -1, :]) / temperature\n","            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n","            outputs = torch.cat([outputs, next_token], dim=1)\n","\n","        return outputs\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = CNNTransformer(vocab_size=len(vocab)).to(device)\n","\n","# Optimizer and loss\n","# Replace criterion with manual cross entropy\n","class StableCrossEntropy(nn.Module):\n","    def __init__(self, ignore_index=-100):\n","        super().__init__()\n","        self.ignore_index = ignore_index\n","\n","    def forward(self, logits, targets):\n","        # Add numerical stability\n","        logits = logits - logits.max(dim=-1, keepdim=True)[0]\n","        log_probs = torch.log_softmax(logits, dim=-1)\n","\n","        # Filter ignored indices\n","        mask = targets != self.ignore_index\n","        log_probs = log_probs[mask]\n","        targets = targets[mask]\n","\n","        return -log_probs.gather(1, targets.unsqueeze(1)).mean()\n","\n","# Replace criterion\n","criterion = StableCrossEntropy(ignore_index=0)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n","#criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n","\n","best_loss = float('inf')\n","for epoch in range(100):\n","    # Training\n","    model.train()\n","    train_loss = 0\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Shift labels for teacher forcing\n","        decoder_input = labels[:, :-1]\n","        decoder_target = labels[:, 1:]\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(images, decoder_input)\n","\n","        # Reshape for loss calculation\n","        outputs = outputs.view(-1, outputs.size(-1))\n","        targets = decoder_target.contiguous().view(-1)\n","\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","        if torch.isnan(loss):\n","          print(\"NaN in model outputs!\")\n","          continue\n","        train_loss += loss.item()\n","\n","    # Validation\n","    model.eval()\n","    test_loss = 0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            decoder_input = labels[:, :-1]\n","            decoder_target = labels[:, 1:]\n","            # Generate predictions\n","            outputs = model(images, decoder_input)\n","\n","            # Calculate loss\n","            outputs = outputs.view(-1, outputs.size(-1))\n","            targets = decoder_target.contiguous().view(-1)\n","            loss = criterion(outputs, targets)\n","            if torch.isnan(loss):\n","              print(\"NaN in model outputs!\")\n","              continue\n","            test_loss += loss.item()\n","\n","    # Update scheduler\n","    avg_train_loss = train_loss / len(train_loader)\n","    avg_test_loss = test_loss / len(test_loader)\n","    scheduler.step(avg_test_loss)\n","\n","    print(f\"Epoch {epoch+1}/100\")\n","    print(f\"Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n","\n","    # Save best model\n","    if avg_test_loss < best_loss:\n","        best_loss = avg_test_loss\n","        #torch.save(model.state_dict(), 'best_transformer_model.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":893},"id":"rZjs9-3cQXjD","executionInfo":{"status":"error","timestamp":1743970640911,"user_tz":300,"elapsed":8404,"user":{"displayName":"Ajitesh Parthasarathy","userId":"10718185110510952821"}},"outputId":"bb06ef99-a4f3-4920-d997-e84dbf4ccfc0"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","Epoch 1/100\n","Train Loss: 5.6560 | Test Loss: 5.5240\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n","NaN in model outputs!\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-9693ce9861d2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Reshape for loss calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-47-9693ce9861d2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, captions)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Encode images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mimg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, 512]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mimg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, 1, 512]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2820\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2822\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2823\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"g4xtljXwYfW3","executionInfo":{"status":"ok","timestamp":1743966408534,"user_tz":300,"elapsed":6,"user":{"displayName":"Ajitesh Parthasarathy","userId":"10718185110510952821"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NN43G2TPZrBC","executionInfo":{"status":"ok","timestamp":1743966408539,"user_tz":300,"elapsed":2,"user":{"displayName":"Ajitesh Parthasarathy","userId":"10718185110510952821"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MdLQSUCZbkdJ","executionInfo":{"status":"ok","timestamp":1743966408556,"user_tz":300,"elapsed":15,"user":{"displayName":"Ajitesh Parthasarathy","userId":"10718185110510952821"}}},"execution_count":7,"outputs":[]}]}