{"cells":[{"cell_type":"markdown","metadata":{"id":"-Raw2ptFPaNR"},"source":["# Install Necessary Libraries for the Project"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CiOGaCTPRPw"},"outputs":[],"source":["!pip install torch datasets accelerate trl"]},{"cell_type":"markdown","metadata":{"id":"lEVO20NNMhSh"},"source":["# Load and Prepare the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3259,"status":"ok","timestamp":1745793012421,"user":{"displayName":"Anthony Brogni","userId":"04470910975254961677"},"user_tz":300},"id":"YOJEpPGuEoVw","outputId":"92f5553d-a9ba-46ef-cdbc-327f9b967e2b"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/kumarv/parth057/anaconda3/envs/WSTATT_Proj/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import math\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from PIL import Image\n","from collections import defaultdict\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torchvision.transforms import functional as TF\n","import random\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0A9O9lUCM83_"},"outputs":[],"source":["class AugmentationTransforms:\n","    \"\"\"Custom data augmentation for handwritten math images\"\"\"\n","\n","    @staticmethod\n","    def random_rotate(image, max_angle=5):\n","        \"\"\"Randomly rotate image by small angle\"\"\"\n","        angle = random.uniform(-max_angle, max_angle)\n","        return TF.rotate(image, angle)\n","\n","    @staticmethod\n","    def random_scale(image, scale_range=(0.9, 1.1)):\n","        \"\"\"Randomly scale image\"\"\"\n","        scale = random.uniform(scale_range[0], scale_range[1])\n","        orig_size = image.size\n","        scaled_size = (int(orig_size[0] * scale), int(orig_size[1] * scale))\n","        image = TF.resize(image, scaled_size)\n","        # Resize back to original size\n","        image = TF.resize(image, orig_size)\n","        return image\n","\n","#     @staticmethod\n","#     def add_noise(image, noise_factor=0.05):\n","#         \"\"\"Add random noise to image\"\"\"\n","#         image_tensor = TF.to_tensor(image)\n","#         noise = torch.randn_like(image_tensor) * noise_factor\n","#         noisy_image = torch.clamp(image_tensor + noise, 0, 1)\n","#         return TF.to_pil_image(noisy_image)\n","\n","#     @staticmethod\n","#     def adjust_contrast(image, factor_range=(0.8, 1.2)):\n","#         \"\"\"Randomly adjust contrast\"\"\"\n","#         factor = random.uniform(factor_range[0], factor_range[1])\n","#         return TF.adjust_contrast(image, factor)\n","\n","# Enhanced Dataset with data augmentation\n","class EnhancedHandwrittenMathDataset(Dataset):\n","    def __init__(self, image_directory, labels_file, transform=None, augment=False):\n","        self.image_paths = []\n","        self.latex_sequences = []\n","        self.transform = transform\n","        self.augment = augment\n","\n","        try:\n","            with open(labels_file, 'r', encoding='utf-8') as f:\n","                for line_num, line in enumerate(f, 1):\n","                    try:\n","                        line = line.strip()\n","                        if not line:\n","                            continue\n","\n","                        parts = line.split('\\t')\n","\n","                        # Handle cases with more than 2 parts\n","                        if len(parts) > 2:\n","                            image_filename, latex_seq = parts[0], '  '.join(parts[1:])\n","                            image_path = os.path.join(image_directory, image_filename)\n","                            if os.path.exists(image_path):\n","                                self.image_paths.append(image_path)\n","                                self.latex_sequences.append(latex_seq)\n","                            else:\n","                                print(f\"Warning: Image file not found at line {line_num}: {image_path}\")\n","\n","                        # Handle cases with exactly 2 parts\n","                        elif len(parts) == 2:\n","                            image_filename, latex_seq = parts\n","                            image_path = os.path.join(image_directory, image_filename)\n","                            if os.path.exists(image_path):\n","                                self.image_paths.append(image_path)\n","                                self.latex_sequences.append(latex_seq)\n","                            else:\n","                                print(f\"Warning: Image file not found at line {line_num}: {image_path}\")\n","\n","                        # Handle cases with insufficient parts\n","                        else:\n","                            print(f\"Warning: Skipping malformed line {line_num}: {line}\")\n","\n","                    except Exception as e:\n","                        print(f\"Error processing line {line_num}: {str(e)}\")\n","                        continue\n","\n","            if not self.image_paths:\n","                print(f\"Warning: No valid image-latex pairs found in {labels_file}\")\n","\n","        except FileNotFoundError:\n","            raise FileNotFoundError(f\"Labels file not found: {labels_file}\")\n","        except PermissionError:\n","            raise PermissionError(f\"Permission denied when accessing file: {labels_file}\")\n","        except Exception as e:\n","            raise Exception(f\"Error reading labels file: {str(e)}\")\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","\n","        try:\n","            image = Image.open(image_path).convert('L')  # Convert to grayscale\n","\n","            # Apply augmentations if enabled\n","            if self.augment:\n","                # Apply random augmentations with 25% probability\n","                if random.random() > 0.25:\n","                    image = AugmentationTransforms.random_rotate(image)\n","                if random.random() > 0.25:\n","                    image = AugmentationTransforms.random_scale(image)\n","                # if random.random() > 0.25:\n","                #     image = AugmentationTransforms.add_noise(image)\n","                # if random.random() > 0.25:\n","                #     image = AugmentationTransforms.adjust_contrast(image)\n","\n","            # Apply standard transforms\n","            if self.transform:\n","                image = self.transform(image)\n","\n","            latex_seq = self.latex_sequences[idx]\n","            return image, latex_seq\n","\n","        except Exception as e:\n","            print(f\"Error loading image {image_path}: {str(e)}\")\n","            # Return a default image and empty sequence in case of error\n","            # This prevents training from crashing if a single image has issues\n","            dummy_image = torch.zeros((1, 224, 224)) if self.transform else Image.new('L', (224, 224))\n","            return dummy_image, \"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVZXX3hVAoAw"},"outputs":[],"source":["# Improved Vocabulary Builder\n","\n","def improved_build_vocab(labels_file, vocab_size=1000):\n","    \"\"\"\n","    Build vocabulary from labels with improved error handling\n","\n","    Args:\n","        labels_file: Path to labels file\n","        vocab_size: Maximum vocabulary size\n","\n","    Returns:\n","        dict: Vocabulary mapping tokens to indices\n","    \"\"\"\n","    # Initialize collections\n","    all_chars = defaultdict(int)\n","    special_tokens = ['<pad>', '<start>', '<end>', '<unk>']\n","\n","    try:\n","        # Read and process the labels file\n","        with open(labels_file, 'r', encoding='utf-8') as f:\n","            for line_num, line in enumerate(f, 1):\n","                try:\n","                    line = line.strip()\n","                    if not line:\n","                        continue\n","\n","                    parts = line.split('\\t')\n","                    if len(parts) > 2:\n","                        latex_seq = '  '.join(parts[1:])\n","                    elif len(parts) == 2:\n","                        latex_seq = parts[1]\n","                    else:\n","                        print(f\"Warning: Skipping malformed line {line_num}: {line}\")\n","                        continue\n","\n","                    # Count character frequencies\n","                    for char in latex_seq:\n","                        all_chars[char] += 1\n","\n","                except Exception as e:\n","                    print(f\"Error processing line {line_num}: {str(e)}\")\n","                    continue\n","\n","    except Exception as e:\n","        raise Exception(f\"Error reading labels file: {str(e)}\")\n","\n","    # Sort characters by frequency (descending)\n","    sorted_chars = sorted(all_chars.items(), key=lambda x: x[1], reverse=True)\n","\n","    # Create vocabulary with special tokens first\n","    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n","\n","    # Add remaining characters up to vocab_size\n","    current_idx = len(special_tokens)\n","    for char, _ in sorted_chars:\n","        if char not in vocab and current_idx < vocab_size:\n","            vocab[char] = current_idx\n","            current_idx += 1\n","\n","    print(f\"Built vocabulary with {len(vocab)} tokens\")\n","    return vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"415y0hiC9W4W"},"outputs":[],"source":["# Improved String-Tensor Conversion Functions\n","\n","def improved_string_to_tensor(string_list, vocab):\n","    \"\"\"\n","    Convert a list of strings to a padded tensor with improved error handling.\n","    \"\"\"\n","    if isinstance(string_list, str):\n","        string_list = [string_list]\n","\n","    all_indices = []\n","    for string in string_list:\n","        indices = [vocab['<start>']] + [vocab.get(char, vocab['<unk>']) for char in string] + [vocab['<end>']]\n","        all_indices.append(torch.tensor(indices, dtype=torch.long))\n","\n","    # Fix: Set batch_first=True\n","    padded_indices = pad_sequence(all_indices, batch_first=True, padding_value=vocab['<pad>'])\n","\n","    # Create padding mask: 1 = pad token\n","    padding_mask = padded_indices.eq(vocab['<pad>'])\n","\n","    return padded_indices, padding_mask\n","\n","def improved_tensor_to_string(tensor, vocab):\n","    \"\"\"\n","    Convert a tensor of token indices to strings with improved handling\n","\n","    Args:\n","        tensor: Tensor of shape [seq_len, batch_size] or [batch_size, seq_len]\n","        vocab: Dictionary mapping tokens to indices\n","\n","    Returns:\n","        list: List of decoded strings\n","    \"\"\"\n","    # Get index-to-token mapping\n","    idx_to_token = {idx: token for token, idx in vocab.items()}\n","\n","    # If tensor is [seq_len, batch_size], convert to [batch_size, seq_len]\n","    if tensor.dim() == 2 and tensor.shape[0] < tensor.shape[1]:\n","        tensor = tensor.transpose(0, 1)\n","\n","    # Ensure tensor is on CPU\n","    tensor = tensor.cpu()\n","\n","    batch_texts = []\n","    for sequence in tensor:\n","        tokens = []\n","        for idx in sequence:\n","            token = idx_to_token.get(idx.item(), \"\")\n","            # Break at end token\n","            if token == \"<end>\":\n","                break\n","            # Skip special tokens\n","            if token not in [\"<pad>\", \"<start>\", \"<unk>\"]:\n","                tokens.append(token)\n","\n","        text = \"\".join(tokens)\n","        batch_texts.append(text)\n","\n","    return batch_texts"]},{"cell_type":"markdown","metadata":{"id":"HYI3-SoWPt7p"},"source":["# Project Code"]},{"cell_type":"markdown","metadata":{"id":"xIeaRfRanCuG"},"source":["Step 1.) CNN or Transformer based Image -> Latex conversion"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"elapsed":2143,"status":"error","timestamp":1745795592924,"user":{"displayName":"Anthony Brogni","userId":"04470910975254961677"},"user_tz":300},"id":"Nut_PzM_EoVy","outputId":"759df2cb-e810-4543-eaaa-2b26d985e658"},"outputs":[{"name":"stdout","output_type":"stream","text":["Built vocabulary with 95 tokens\n","Vocabulary size: 95\n","Training on cuda\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50 [Train]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [02:31<00:00,  4.52it/s, loss=1.2624]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50 -> Avg Train Loss: 1.5068\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 2/50 [Train]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:56<00:00,  5.89it/s, loss=0.9280]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/50 -> Avg Train Loss: 0.7475\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 3/50 [Train]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:58<00:00,  5.79it/s, loss=0.5427]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/50 -> Avg Train Loss: 0.4907\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 4/50 [Train]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:56<00:00,  5.86it/s, loss=1.1707]"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/50 -> Avg Train Loss: 0.3609\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 5/50 [Train]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.81it/s, loss=0.3830]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/50 -> Avg Train Loss: 0.2776\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 6/50 [Train]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.81it/s, loss=0.3668]"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/50 -> Avg Train Loss: 0.2209\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 7/50 [Train]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.84it/s, loss=0.2659]"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/50 -> Avg Train Loss: 0.1783\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 8/50 [Train]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.83it/s, loss=0.1257]"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/50 -> Avg Train Loss: 0.1458\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 9/50 [Train]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:59<00:00,  5.72it/s, loss=0.5060]"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/50 -> Avg Train Loss: 0.1251\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 10/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.84it/s, loss=0.6985]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/50 -> Avg Train Loss: 0.1067\n","Saved model checkpoint to improved_checkpoints/model_epoch_10_train_loss_0.1067.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 11/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.82it/s, loss=0.1319]"]},{"name":"stdout","output_type":"stream","text":["Epoch 11/50 -> Avg Train Loss: 0.0925\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 12/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:59<00:00,  5.74it/s, loss=0.0782]"]},{"name":"stdout","output_type":"stream","text":["Epoch 12/50 -> Avg Train Loss: 0.0826\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 13/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:56<00:00,  5.86it/s, loss=0.1328]"]},{"name":"stdout","output_type":"stream","text":["Epoch 13/50 -> Avg Train Loss: 0.0738\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 14/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.81it/s, loss=0.2947]"]},{"name":"stdout","output_type":"stream","text":["Epoch 14/50 -> Avg Train Loss: 0.0667\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 15/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.82it/s, loss=0.3132]"]},{"name":"stdout","output_type":"stream","text":["Epoch 15/50 -> Avg Train Loss: 0.0608\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 16/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:58<00:00,  5.76it/s, loss=0.4773]"]},{"name":"stdout","output_type":"stream","text":["Epoch 16/50 -> Avg Train Loss: 0.0568\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 17/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:56<00:00,  5.87it/s, loss=0.7286]"]},{"name":"stdout","output_type":"stream","text":["Epoch 17/50 -> Avg Train Loss: 0.0529\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 18/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.83it/s, loss=0.6702]"]},{"name":"stdout","output_type":"stream","text":["Epoch 18/50 -> Avg Train Loss: 0.0498\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 19/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.83it/s, loss=0.1282]"]},{"name":"stdout","output_type":"stream","text":["Epoch 19/50 -> Avg Train Loss: 0.0457\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 20/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.82it/s, loss=0.2031]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 20/50 -> Avg Train Loss: 0.0439\n","Saved model checkpoint to improved_checkpoints/model_epoch_20_train_loss_0.0439.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 21/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.81it/s, loss=0.1699]"]},{"name":"stdout","output_type":"stream","text":["Epoch 21/50 -> Avg Train Loss: 0.0405\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 22/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.84it/s, loss=0.2772]"]},{"name":"stdout","output_type":"stream","text":["Epoch 22/50 -> Avg Train Loss: 0.0391\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 23/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.85it/s, loss=0.1456]"]},{"name":"stdout","output_type":"stream","text":["Epoch 23/50 -> Avg Train Loss: 0.0369\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 24/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.85it/s, loss=0.3717]"]},{"name":"stdout","output_type":"stream","text":["Epoch 24/50 -> Avg Train Loss: 0.0359\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 25/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:56<00:00,  5.86it/s, loss=0.2860]"]},{"name":"stdout","output_type":"stream","text":["Epoch 25/50 -> Avg Train Loss: 0.0339\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 26/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.81it/s, loss=0.0426]"]},{"name":"stdout","output_type":"stream","text":["Epoch 26/50 -> Avg Train Loss: 0.0321\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 27/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:58<00:00,  5.80it/s, loss=0.1668]"]},{"name":"stdout","output_type":"stream","text":["Epoch 27/50 -> Avg Train Loss: 0.0316\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 28/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.83it/s, loss=0.1057]"]},{"name":"stdout","output_type":"stream","text":["Epoch 28/50 -> Avg Train Loss: 0.0307\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 29/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:58<00:00,  5.79it/s, loss=0.2307]"]},{"name":"stdout","output_type":"stream","text":["Epoch 29/50 -> Avg Train Loss: 0.0298\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 30/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:58<00:00,  5.77it/s, loss=0.2267]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 30/50 -> Avg Train Loss: 0.0288\n","Saved model checkpoint to improved_checkpoints/model_epoch_30_train_loss_0.0288.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 31/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:58<00:00,  5.78it/s, loss=0.1351]"]},{"name":"stdout","output_type":"stream","text":["Epoch 31/50 -> Avg Train Loss: 0.0279\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 32/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.84it/s, loss=0.2484]"]},{"name":"stdout","output_type":"stream","text":["Epoch 32/50 -> Avg Train Loss: 0.0272\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 33/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.81it/s, loss=0.9343]"]},{"name":"stdout","output_type":"stream","text":["Epoch 33/50 -> Avg Train Loss: 0.0273\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 34/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:56<00:00,  5.87it/s, loss=0.1423]"]},{"name":"stdout","output_type":"stream","text":["Epoch 34/50 -> Avg Train Loss: 0.0245\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 35/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:58<00:00,  5.79it/s, loss=1.5036]"]},{"name":"stdout","output_type":"stream","text":["Epoch 35/50 -> Avg Train Loss: 0.0275\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 36/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:59<00:00,  5.75it/s, loss=0.0232]"]},{"name":"stdout","output_type":"stream","text":["Epoch 36/50 -> Avg Train Loss: 0.0241\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 37/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.85it/s, loss=0.2077]"]},{"name":"stdout","output_type":"stream","text":["Epoch 37/50 -> Avg Train Loss: 0.0240\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 38/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:58<00:00,  5.80it/s, loss=0.2301]"]},{"name":"stdout","output_type":"stream","text":["Epoch 38/50 -> Avg Train Loss: 0.0237\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 39/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:59<00:00,  5.73it/s, loss=0.2122]"]},{"name":"stdout","output_type":"stream","text":["Epoch 39/50 -> Avg Train Loss: 0.0219\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 40/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.84it/s, loss=0.1497]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 40/50 -> Avg Train Loss: 0.0218\n","Saved model checkpoint to improved_checkpoints/model_epoch_40_train_loss_0.0218.pt\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 41/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.85it/s, loss=0.0141]"]},{"name":"stdout","output_type":"stream","text":["Epoch 41/50 -> Avg Train Loss: 0.0207\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 42/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.84it/s, loss=0.0350]"]},{"name":"stdout","output_type":"stream","text":["Epoch 42/50 -> Avg Train Loss: 0.0204\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 43/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:56<00:00,  5.86it/s, loss=0.1109]"]},{"name":"stdout","output_type":"stream","text":["Epoch 43/50 -> Avg Train Loss: 0.0211\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 44/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:56<00:00,  5.86it/s, loss=0.0885]"]},{"name":"stdout","output_type":"stream","text":["Epoch 44/50 -> Avg Train Loss: 0.0201\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 45/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:59<00:00,  5.74it/s, loss=0.1803]"]},{"name":"stdout","output_type":"stream","text":["Epoch 45/50 -> Avg Train Loss: 0.0203\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 46/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:58<00:00,  5.78it/s, loss=0.0529]"]},{"name":"stdout","output_type":"stream","text":["Epoch 46/50 -> Avg Train Loss: 0.0189\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 47/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.84it/s, loss=0.2081]"]},{"name":"stdout","output_type":"stream","text":["Epoch 47/50 -> Avg Train Loss: 0.0188\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 48/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.83it/s, loss=0.0060]"]},{"name":"stdout","output_type":"stream","text":["Epoch 48/50 -> Avg Train Loss: 0.0191\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 49/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:57<00:00,  5.83it/s, loss=0.0306]"]},{"name":"stdout","output_type":"stream","text":["Epoch 49/50 -> Avg Train Loss: 0.0182\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch 50/50 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:58<00:00,  5.78it/s, loss=0.1674]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 50/50 -> Avg Train Loss: 0.0175\n","Saved model checkpoint to improved_checkpoints/model_epoch_50_train_loss_0.0175.pt\n","Training finished.\n"]}],"source":["import traceback\n","\n","# Improved Encoder using ResNet Backbone\n","class ImprovedCNNEncoder(nn.Module):\n","    def __init__(self, output_channels=256):\n","        super(ImprovedCNNEncoder, self).__init__()\n","        # Use pretrained ResNet but adapt for grayscale images\n","        resnet = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n","\n","        # Modify first conv layer to accept grayscale input (1 channel)\n","        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","\n","        # Initialize weights with first channel of pretrained model\n","        with torch.no_grad():\n","            self.conv1.weight.data = resnet.conv1.weight.data.mean(dim=1, keepdim=True)\n","\n","        # Use remaining ResNet layers\n","        self.bn1 = resnet.bn1\n","        self.relu = resnet.relu\n","        self.maxpool = resnet.maxpool\n","        self.layer1 = resnet.layer1\n","        self.layer2 = resnet.layer2\n","        self.layer3 = resnet.layer3\n","        self.layer4 = resnet.layer4\n","\n","        # Project to the desired output dimension\n","        self.proj = nn.Conv2d(512, output_channels, kernel_size=1)\n","\n","        # Adaptive pooling to ensure consistent output size\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n","\n","    def forward(self, x):\n","        # Forward pass through ResNet layers\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        # Project to desired channels\n","        x = self.proj(x)\n","\n","        # Ensure consistent spatial dims\n","        x = self.adaptive_pool(x)\n","\n","        # Reshape for transformer input [B, C, H, W] -> [B, H*W, C]\n","        batch_size, channels, height, width = x.size()\n","        x = x.view(batch_size, channels, -1).permute(0, 2, 1)\n","\n","        return x\n","\n","# Improved Transformer with Multi-Head Attention\n","class ImprovedTransformerDecoder(nn.Module):\n","    def __init__(self, vocab_size, d_model=512, num_layers=6, nhead=8, dim_feedforward=2048, dropout=0.1, max_seq_length=200):\n","        super(ImprovedTransformerDecoder, self).__init__()\n","        self.d_model = d_model\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_embedding = nn.Embedding(max_seq_length, d_model)\n","\n","        # Use PyTorch's TransformerDecoder with more layers\n","        decoder_layer = nn.TransformerDecoderLayer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout\n","        )\n","        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n","\n","        # Output projection\n","        self.fc_out = nn.Linear(d_model, vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, tgt, memory, tgt_mask=None, tgt_padding_mask=None):\n","        # tgt: [T, B] tensor of token indices\n","        seq_len, batch_size = tgt.size()\n","\n","        # Create position indices and clamp them to valid range\n","        positions = torch.arange(0, seq_len).unsqueeze(1).expand(seq_len, batch_size).to(tgt.device)\n","        positions = positions.clamp(0, self.pos_embedding.num_embeddings - 1)\n","\n","        # Embedding with positional encoding\n","        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model) + self.pos_embedding(positions)\n","        tgt_emb = self.dropout(tgt_emb)\n","\n","        # Create mask if not provided\n","        if tgt_mask is None:\n","          tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len, dtype=torch.bool).to(tgt.device)\n","\n","        # Forward through transformer\n","        output = self.transformer_decoder(\n","            tgt_emb, memory,\n","            tgt_mask=tgt_mask,\n","            tgt_key_padding_mask=tgt_padding_mask\n","        )\n","\n","        # Project to vocabulary\n","        output = self.fc_out(output)\n","\n","        return output\n","\n","# Combined Improved Model with Beam Search\n","class ImprovedHandwrittenMathToLatexModel(nn.Module):\n","    def __init__(self, vocab_size, d_model=512):\n","        super(ImprovedHandwrittenMathToLatexModel, self).__init__()\n","        self.encoder = ImprovedCNNEncoder(output_channels=d_model)\n","        self.decoder = ImprovedTransformerDecoder(vocab_size=vocab_size, d_model=d_model)\n","\n","    def forward(self, images, tgt_seq, tgt_padding_mask=None):\n","        # images: [B, 1, H, W]\n","        # tgt_seq: [T, B]\n","        enc_out = self.encoder(images)  # [B, N, d_model]\n","\n","        # Transpose to [N, B, d_model] for transformer\n","        enc_out = enc_out.permute(1, 0, 2)\n","\n","        # Forward through decoder\n","        output = self.decoder(tgt_seq, enc_out, tgt_padding_mask=tgt_padding_mask)\n","\n","        return output\n","\n","    def beam_search_decode(self, image, vocab, beam_size=5, max_len=100):\n","        \"\"\"\n","        Perform beam search to generate LaTeX sequence.\n","\n","        Args:\n","            image: either a [C, H, W] tensor or a [1, C, H, W] tensor\n","            vocab: dict mapping tokens → indices (must include '<start>' and '<end>')\n","            beam_size: number of beams\n","            max_len: max output length\n","        \"\"\"\n","        device = next(self.parameters()).device\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 1) Massage image into shape [1, C, H, W]\n","        # ─────────────────────────────────────────────────────────────────────────\n","        img = image\n","        if img.dim() == 3:\n","            img = img.unsqueeze(0)\n","        while img.dim() > 4 or (img.dim() == 4 and img.size(0) != 1):\n","            img = img.squeeze(0)\n","        assert img.dim() == 4 and img.size(0) == 1, f\"Unrecognized image shape: {img.shape}\"\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 2) Encode\n","        # ─────────────────────────────────────────────────────────────────────────\n","        try:\n","            enc = self.encoder(img.to(device))  # [1, N, E]\n","        except Exception:\n","            print(\"ERROR in encoder call: input shape\", img.shape)\n","            traceback.print_exc()\n","            raise\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 3) Prepare memory: [N, beam_size, E]\n","        # ─────────────────────────────────────────────────────────────────────────\n","        try:\n","            memory = enc.permute(1, 0, 2)             # [N, 1, E]\n","            memory = memory.expand(-1, beam_size, -1)  # [N, beam_size, E]\n","        except Exception:\n","            print(\"ERROR permuting/expanding memory: enc.shape =\", enc.shape)\n","            traceback.print_exc()\n","            raise\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 4) Initialize beams\n","        # ─────────────────────────────────────────────────────────────────────────\n","        start_idx = vocab['<start>']\n","        end_idx   = vocab['<end>']\n","\n","        seqs   = torch.full((1, 1), start_idx, dtype=torch.long, device=device)  # [1,1]\n","        scores = torch.zeros(1, device=device)                                  # [1]\n","\n","        # Replicate initial <start> across beams\n","        if seqs.size(1) == 1 and memory.size(1) > 1:\n","            seqs   = seqs.expand(-1, memory.size(1)).clone()  # [1, beam_size]\n","            scores = scores.expand(memory.size(1))            # [beam_size]\n","\n","        finished_seqs   = []\n","        finished_scores = []\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 5) Beam search loop\n","        # ─────────────────────────────────────────────────────────────────────────\n","        for step in range(max_len):\n","            # 5a) target mask\n","            tgt_len = seqs.size(0)\n","            tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n","                tgt_len, dtype=torch.bool, device=device\n","            )\n","\n","            # 5b) decoder forward\n","            try:\n","                out = self.decoder(seqs, memory, tgt_mask=tgt_mask)  # [T, B, V]\n","            except Exception:\n","                print(f\"ERROR in decoder at step {step}: seqs={seqs.shape}, memory={memory.shape}\")\n","                traceback.print_exc()\n","                raise\n","\n","            # 5c) log-probs\n","            logits   = out[-1, :, :]                 # [B, V]\n","            log_probs= F.log_softmax(logits, dim=-1) # [B, V]\n","            V = log_probs.size(-1)\n","\n","            # 5d) top-k\n","            if step == 0:\n","                topk_scores, topk_idxs = log_probs[0].topk(beam_size, dim=-1)\n","                beam_indices = torch.zeros(beam_size, dtype=torch.long, device=device)\n","            else:\n","                expanded = scores.unsqueeze(1) + log_probs  # [B, V]\n","                topk_scores, flat_idxs = expanded.view(-1).topk(beam_size, dim=-1)\n","                beam_indices = flat_idxs // V\n","                topk_idxs    = flat_idxs % V\n","\n","            # 5e) build new beams\n","            new_seqs   = []\n","            new_scores = []\n","            for b_i, tok_i, sc in zip(beam_indices, topk_idxs, topk_scores):\n","                candidate = torch.cat([seqs[:, b_i], tok_i.unsqueeze(0)])\n","                if tok_i.item() == end_idx:\n","                    finished_seqs.append(candidate)\n","                    finished_scores.append(sc)\n","                else:\n","                    new_seqs.append(candidate.unsqueeze(1))\n","                    new_scores.append(sc)\n","\n","            # 5f) exit if no active beams\n","            if not new_seqs:\n","                break\n","\n","            # 5g) update\n","            seqs   = torch.cat(new_seqs, dim=1)  # [step+1, beam_count]\n","            scores = torch.stack(new_scores)     # [beam_count]\n","\n","            # 5h) reorder memory\n","            keep   = beam_indices[: len(new_seqs)]\n","            memory = memory.index_select(1, keep)\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 6) finalize: if none finished, treat current beams as finished\n","        # ─────────────────────────────────────────────────────────────────────────\n","        if not finished_seqs:\n","            for b in range(seqs.size(1)):\n","                seq = torch.cat([seqs[:, b], torch.tensor([end_idx], device=device)])\n","                finished_seqs.append(seq)\n","                finished_scores.append(scores[b])\n","\n","        # ─────────────────────────────────────────────────────────────────────────\n","        # 7) choose best sequence\n","        # ─────────────────────────────────────────────────────────────────────────\n","        best_idx = torch.tensor(finished_scores).argmax().item()\n","        return finished_seqs[best_idx]\n","\n","# Training with Learning Rate Scheduling\n","def train_with_scheduling(model, train_loader, criterion, vocab, learning_rate=1e-4, num_epochs=5, checkpoint_dir='checkpoints'):\n","    \"\"\"\n","    Train model with learning rate scheduling and tqdm progress bar.\n","    Validation phase removed for faster training.\n","    \"\"\"\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Training on {device}\")\n","\n","    model = model.to(device)\n","    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","    pad_idx = vocab['<pad>']\n","\n","    for epoch in range(num_epochs):\n","        # --- Training phase ---\n","        model.train()\n","        train_loss = 0.0\n","        num_train_batches = len(train_loader)\n","\n","        # Create tqdm progress bar for training loop\n","        # Wrap enumerate(train_loader) and provide total for accurate progress\n","        pbar_train = tqdm(\n","            enumerate(train_loader),\n","            total=num_train_batches,\n","            desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"\n","        )\n","\n","        for batch_idx, (images, target_seqs) in pbar_train: # Iterate over the tqdm object\n","            images = images.to(device)\n","            target_tensor, _ = improved_string_to_tensor(target_seqs, vocab)\n","            target_tensor = target_tensor.to(device)\n","\n","            input_tensor_bfirst = target_tensor[:, :-1]\n","            target_labels_bfirst = target_tensor[:, 1:]\n","            input_tensor = input_tensor_bfirst.transpose(0, 1)\n","            input_padding_mask = (input_tensor_bfirst == pad_idx).to(device)\n","            target_labels = target_labels_bfirst.transpose(0, 1)\n","\n","            # --- Forward pass ---\n","            optimizer.zero_grad()\n","            output = model(images, input_tensor, tgt_padding_mask=input_padding_mask)\n","\n","            # --- Compute loss ---\n","            loss = criterion(output.reshape(-1, output.shape[-1]), target_labels.reshape(-1))\n","\n","            # --- Backward pass ---\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","\n","            current_loss = loss.item()\n","            train_loss += current_loss\n","\n","            # Update progress bar postfix with current batch loss\n","            pbar_train.set_postfix({'loss': f'{current_loss:.4f}'})\n","\n","        # Calculate average training loss for the epoch\n","        avg_train_loss = train_loss / num_train_batches\n","        # Update the postfix one last time to show average loss for the epoch\n","        pbar_train.set_postfix({'avg_loss': f'{avg_train_loss:.4f}'})\n","        pbar_train.close() # Close the training progress bar\n","\n","        # Save checkpoint every nth epoch\n","        n = 10\n","        print(f'Epoch {epoch+1}/{num_epochs} -> Avg Train Loss: {avg_train_loss:.4f}')\n","        if (epoch + 1) % n == 0:\n","          checkpoint_path = os.path.join(\n","              checkpoint_dir,\n","              f'model_epoch_{epoch+1}_train_loss_{avg_train_loss:.4f}.pt'\n","          )\n","          torch.save({\n","              'epoch': epoch + 1,\n","              'model_state_dict': model.state_dict(),\n","              'optimizer_state_dict': optimizer.state_dict(),\n","              'train_loss': avg_train_loss,\n","              'vocab': vocab,\n","          }, checkpoint_path)\n","          print(f'Saved model checkpoint to {checkpoint_path}')\n","\n","    print(\"Training finished.\")\n","\n","# Paths\n","# from google.colab import drive\n","# drive.mount('/content/drive', force_remount=False)\n","# folder_path = '/content/drive/My Drive/Senior Year/Spring Semester/CSCI 5527/CSCI 5527 Project/Data/3312_images'\n","folder_path = '100k_processed'\n","image_directory = os.path.join(folder_path, \"synthetic_images\")\n","labels_file = os.path.join(folder_path, \"synthetic_labels.txt\")\n","checkpoint_dir = 'improved_checkpoints'\n","\n","# Create vocabulary\n","vocab = improved_build_vocab(labels_file, vocab_size=1000)\n","vocab_size = len(vocab)\n","print(f\"Vocabulary size: {vocab_size}\")\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resize for ResNet\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale images\n","])\n","\n","# Create datasets with augmentation for training\n","full_dataset = EnhancedHandwrittenMathDataset(\n","    image_directory=image_directory,\n","    labels_file=labels_file,\n","    transform=transform,\n","    augment=True  # Enable augmentation for training\n",")\n","\n","# Split dataset - we'll just use train and test since we're removing validation\n","train_size = int(0.9 * len(full_dataset))\n","test_size = len(full_dataset) - train_size\n","\n","train_dataset, test_dataset = random_split(\n","    full_dataset,\n","    [train_size, test_size],\n","    generator=torch.Generator().manual_seed(42)\n",")\n","\n","# Disable augmentation for test dataset\n","test_dataset.dataset.augment = False\n","\n","# Create dataloaders\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=1)\n","\n","# Create model\n","model = ImprovedHandwrittenMathToLatexModel(vocab_size=vocab_size)\n","\n","# Define loss function that ignores padding\n","criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n","\n","# Train model with validation removed\n","train_with_scheduling(\n","    model=model,\n","    train_loader=train_loader,\n","    criterion=criterion,\n","    vocab=vocab,\n","    learning_rate=1e-4,\n","    num_epochs=50,\n","    checkpoint_dir=checkpoint_dir,\n",")"]},{"cell_type":"markdown","metadata":{"id":"lJPbIDhILlK4"},"source":["**Evaluate the trained model and save predictions to a csv file**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208646,"status":"ok","timestamp":1745795813269,"user":{"displayName":"Anthony Brogni","userId":"04470910975254961677"},"user_tz":300},"id":"oqI6OLsBEoVy","outputId":"a64a9afd-e794-4c70-b199-8175e0adc0d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluating on device: cuda\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_466753/4107680350.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(checkpoint_path, map_location=device)\n"]},{"name":"stdout","output_type":"stream","text":["Loaded checkpoint: epoch 40  train_loss=0.0218\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating [Test]: 100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [00:07<00:00,  9.92it/s, batch_loss=0.0773, acc=98.08%]\n","Decoding & Saving:   4%|████▍                                                                                                             | 3/77 [01:17<31:34, 25.61s/it]"]}],"source":["import os\n","import torch\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import pandas as pd\n","\n","# ─────────────────────────────────────────────────────────────────────────────\n","# 1. Configuration\n","# ─────────────────────────────────────────────────────────────────────────────\n","checkpoint_dir  = 'improved_checkpoints'\n","checkpoint_name = 'model_epoch_40_train_loss_0.0218.pt'\n","checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n","\n","# Assume test_loader, vocab, and improved_string_to_tensor are already defined\n","pad_idx = vocab['<pad>']\n","\n","# ─────────────────────────────────────────────────────────────────────────────\n","# 2. Model & checkpoint loading\n","# ─────────────────────────────────────────────────────────────────────────────\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Evaluating on device: {device}\")\n","\n","model = ImprovedHandwrittenMathToLatexModel(vocab_size=len(vocab), d_model=512)\n","model = model.to(device)\n","\n","ckpt = torch.load(checkpoint_path, map_location=device)\n","model.load_state_dict(ckpt['model_state_dict'])\n","model.eval()\n","print(f\"Loaded checkpoint: epoch {ckpt['epoch']}  train_loss={ckpt['train_loss']:.4f}\")\n","\n","# ─────────────────────────────────────────────────────────────────────────────\n","# 3. Evaluation with tqdm\n","# ─────────────────────────────────────────────────────────────────────────────\n","total_loss      = 0.0\n","total_tokens    = 0\n","correct_tokens  = 0\n","criterion       = torch.nn.CrossEntropyLoss(ignore_index=pad_idx, reduction='sum')\n","\n","with torch.no_grad():\n","    pbar = tqdm(test_loader, desc=\"Evaluating [Test]\")\n","    for images, target_seqs in pbar:\n","        # Prepare tensors\n","        images = images.to(device)\n","        target_tensor, _ = improved_string_to_tensor(target_seqs, vocab)\n","        target_tensor = target_tensor.to(device)\n","\n","        input_tensor    = target_tensor[:, :-1].transpose(0,1)\n","        labels          = target_tensor[:,  1:].transpose(0,1)\n","        padding_mask    = (target_tensor[:, :-1] == pad_idx).to(device)\n","\n","        # Forward\n","        outputs = model(images, input_tensor, tgt_padding_mask=padding_mask)\n","        # outputs: [T-1, B, V]\n","\n","        # Loss\n","        logits_flat = outputs.reshape(-1, outputs.size(-1))\n","        labels_flat = labels.reshape(-1)\n","        loss        = criterion(logits_flat, labels_flat)\n","        total_loss += loss.item()\n","\n","        # Token‐level accuracy\n","        preds        = logits_flat.argmax(dim=-1)\n","        mask         = labels_flat != pad_idx\n","        correct      = (preds[mask] == labels_flat[mask]).sum().item()\n","        total_tokens += mask.sum().item()\n","        correct_tokens += correct\n","\n","        # Update tqdm postfix\n","        pbar.set_postfix({\n","            'batch_loss': f\"{(loss.item()/mask.sum().item()):.4f}\",\n","            'acc':        f\"{(correct/mask.sum().item()*100):.2f}%\"\n","        })\n","\n","# Build reverse vocab\n","idx_to_token = {idx: tok for tok, idx in vocab.items()}\n","\n","def tensor_to_string(tensor_seq, idx_to_token):\n","    \"\"\"Convert a 1D LongTensor of token IDs (including <start> and <end>) to a string.\"\"\"\n","    tokens = []\n","    for idx in tensor_seq.cpu().tolist():\n","        tok = idx_to_token.get(idx, '<unk>')\n","        if tok in ['<start>', '<end>', '<pad>']:\n","            continue\n","        tokens.append(tok)\n","    return ' '.join(tokens)\n","\n","# Lists to collect\n","decoded_list = []\n","reference_list = []\n","\n","model.eval()\n","with torch.no_grad():\n","    pbar = tqdm(test_loader, desc=\"Decoding & Saving\")\n","    for images, target_seqs in pbar:\n","        images = images.to(device)  # [B,1,H,W]\n","        for img_tensor, ref_str in zip(images, target_seqs):\n","            # img_tensor arrives as [1, C, H, W] or maybe [C, H, W]\n","            img = img_tensor\n","\n","            # 1) If it has a leading batch dim of 1 (shape [1, C, H, W]), remove it:\n","            if img.dim() == 4 and img.size(0) == 1:\n","                img = img.squeeze(0)\n","\n","            # 2) If it somehow has two batch dims ([1, 1, C, H, W]), remove both:\n","            while img.dim() > 3:\n","                img = img.squeeze(0)\n","\n","            # Now img.dim() should be exactly 3: [C, H, W]\n","            assert img.dim() == 3, f\"Expected 3D image, got {img.shape}\"\n","\n","            # Move to device\n","            img = img.to(device)\n","\n","            # Decode\n","            pred_seq = model.beam_search_decode(img, vocab, beam_size=5, max_len=100)\n","\n","            # Convert to string\n","            decoded_str = tensor_to_string(pred_seq, idx_to_token)\n","            decoded_list.append(decoded_str)\n","            reference_list.append(ref_str)\n","\n","# Save to CSV\n","df = pd.DataFrame({'prediction': decoded_list, 'reference': reference_list})\n","csv_path = 'test_predictions2_40.csv'\n","df.to_csv(csv_path, index=False)\n","print(f\"Saved predictions to {csv_path}\")\n","print(df.head())\n","\n","# ─────────────────────────────────────────────────────────────────────────────\n","# 4. Final metrics\n","# ─────────────────────────────────────────────────────────────────────────────\n","avg_loss = total_loss / total_tokens\n","tok_acc  = correct_tokens / total_tokens * 100\n","\n","print(f\"\\nTest set → Avg per-token loss: {avg_loss:.4f} | Token accuracy: {tok_acc:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"wK1-Y3aWnTsa"},"source":["**Step 2.) Finetune an LLM using GRPO training to correct errors in the Latex syntax**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46,"status":"ok","timestamp":1745797078840,"user":{"displayName":"Anthony Brogni","userId":"04470910975254961677"},"user_tz":300},"id":"KAU-iLliEoVz","outputId":"2632b7e8-04fe-4e52-bd6c-bcab3550f6e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                          prediction  \\\n","0        w _ { 1 } = \\ f r a c { L } { m r ^ { 2 } }   \n","1  A   =   3 \\ s q r t { 2 5 + 1 5 \\ s q r t { 2 ...   \n","2                          W   =   \\ i n t   F   d X   \n","3  \\ l e f t ( \\ f r a c { 5 } { \\ s q r t { 7 } ...   \n","4            d   =   \\ s q r t { h ( 2 R   +   h ) }   \n","\n","                                           reference  \n","0                    \\omega_{1}=\\frac{L_{1}}{mr^{2}}  \n","1                           A=3\\sqrt{25+10\\sqrt5a^2}  \n","2                                      W = \\int F dx  \n","3  \\left(\\frac{5}{\\sqrt{7}}\\right)^{4}\\cdot\\frac{...  \n","4                               d = \\sqrt{h(2R + h)}  \n"]}],"source":["import pandas as pd\n","csv_path = 'test_predictions2.csv'\n","df = pd.read_csv(csv_path)\n","print(df.head())  # show the first few rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hjuC982EoVz","outputId":"6a22322f-e540-4328-a6fe-2ece320da8f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting train_grpo.py\n"]},{"ename":"PermissionError","evalue":"[Errno 13] Permission denied: 'train_grpo.py'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_cell_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwritefile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_grpo.py\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mimport os\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mimport torch\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mimport numpy as np\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfrom trl import GRPOConfig, GRPOTrainer\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfrom rapidfuzz import fuzz\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfrom datasets import Dataset\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfrom datetime import datetime\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mimport pandas as pd\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# Create the prompts for GRPO Training\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdef create_prompt(example):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    example[\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m] = f\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure that the following text is valid LaTeX by fixing syntax issues as needed. Here is the potentially invalid LaTeX: \u001b[39m\u001b[38;5;132;01m{example[\"prediction\"]}\u001b[39;00m\u001b[38;5;124m. What is the fixed valid LaTeX: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    return example\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# Read in the dataset to use from csv file\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mcsv_path = \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mtest_predictions.csv\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdf = pd.read_csv(csv_path)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdf = df.apply(create_prompt, axis=1)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdataset2 = Dataset.from_pandas(df)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mprint(dataset2)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# Determine device\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdevice = torch.device(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m if torch.cuda.is_available() else \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mprint(f\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining will run on: \u001b[39m\u001b[38;5;132;01m{device}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# Create a unique checkpoint directory for each run using a timestamp\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mrun = datetime.now().strftime(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mcheckpoint_dir = f\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m/home/csci5527/shared/the_gradients/5527/\u001b[39m\u001b[38;5;132;01m{run}\u001b[39;00m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mos.makedirs(checkpoint_dir, exist_ok=True)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdef reward(completions, **kwargs):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward function that rewards a similarity score between two strings in the range [0,1].\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    correct_latex = kwargs[\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    rewards = []\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    for completion, reference in zip(completions, correct_latex):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m      if not completion or not reference:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        rewards.append(0.0)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        continue\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m      # Do not reward empty strings\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m      if len(completion) == 0:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            rewards.append(0.0)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            continue\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m      # Perfect match gets a full reward\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m      if completion == reference:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m          rewards.append(1.0)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m          continue\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m      # Apply RapidFuzz ratio for all cases (handles different lengths well)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m      similarity = fuzz.ratio(completion, reference) / 100.0\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m      # # Add additional penalty for length mismatch\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m      # length_penalty = max(0, 1 - (abs(len(completion) - len(reference)) / max(len(reference), 1)))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m      # # Combined score is a linear combination of similarity and length_penalty\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m      # final_score = (similarity * 0.5) + (length_penalty * 0.5)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m      # rewards.append(final_score)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m      rewards.append(similarity)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    return rewards\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtraining_args = GRPOConfig(\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    output_dir=checkpoint_dir,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    logging_steps=50,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    per_device_train_batch_size=4,  # Decrease this to lower vram usage\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    num_generations=4,  # Decrease this to lower vram usage\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    save_strategy=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,  # Do not save checkpoints (saves storage space)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    bf16=True,  # Enable bf16 mixed precision on A100 GPUs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtrainer = GRPOTrainer(\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    model=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/Phi-4-mini-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    reward_funcs=reward,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    args=training_args,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    train_dataset=dataset2,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# Train and save the final model\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtrainer.train()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtrainer.save_model(os.path.join(checkpoint_dir, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/home/kumarv/parth057/anaconda3/envs/WSTATT_Proj/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n","File \u001b[0;32m/home/kumarv/parth057/anaconda3/envs/WSTATT_Proj/lib/python3.12/site-packages/IPython/core/magics/osm.py:854\u001b[0m, in \u001b[0;36mOSMagics.writefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[1;32m    853\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mappend \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 854\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(filename, mode, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    855\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(cell)\n","\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'train_grpo.py'"]}],"source":["%%writefile train_grpo.py\n","\n","import os\n","import torch\n","import numpy as np\n","from trl import GRPOConfig, GRPOTrainer\n","from rapidfuzz import fuzz\n","from datasets import Dataset\n","from datetime import datetime\n","import pandas as pd\n","\n","# Create the prompts for GRPO Training\n","def create_prompt(example):\n","    example[\"prompt\"] = f\"\"\"Please ensure that the following text is valid LaTeX by fixing syntax issues as needed. Here is the potentially invalid LaTeX: {example[\"prediction\"]}. What is the fixed valid LaTeX: \"\"\"\n","    return example\n","\n","# Read in the dataset to use from csv file\n","csv_path = 'test_predictions.csv'\n","df = pd.read_csv(csv_path)\n","df = df.apply(create_prompt, axis=1)\n","dataset2 = Dataset.from_pandas(df)\n","print(dataset2)\n","\n","# Determine device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Training will run on: {device}\")\n","\n","# Create a unique checkpoint directory for each run using a timestamp\n","run = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","checkpoint_dir = f'/home/csci5527/shared/the_gradients/5527/{run}'\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","def reward(completions, **kwargs):\n","    \"\"\"Reward function that rewards a similarity score between two strings in the range [0,1].\"\"\"\n","    correct_latex = kwargs[\"reference\"]\n","    rewards = []\n","    for completion, reference in zip(completions, correct_latex):\n","      if not completion or not reference:\n","        rewards.append(0.0)\n","        continue\n","      # Do not reward empty strings\n","      if len(completion) == 0:\n","            rewards.append(0.0)\n","            continue\n","      # Perfect match gets a full reward\n","      if completion == reference:\n","          rewards.append(1.0)\n","          continue\n","      # Apply RapidFuzz ratio for all cases (handles different lengths well)\n","      similarity = fuzz.ratio(completion, reference) / 100.0\n","      # # Add additional penalty for length mismatch\n","      # length_penalty = max(0, 1 - (abs(len(completion) - len(reference)) / max(len(reference), 1)))\n","      # # Combined score is a linear combination of similarity and length_penalty\n","      # final_score = (similarity * 0.5) + (length_penalty * 0.5)\n","      # rewards.append(final_score)\n","      rewards.append(similarity)\n","    return rewards\n","\n","training_args = GRPOConfig(\n","    output_dir=checkpoint_dir,\n","    logging_steps=50,\n","    per_device_train_batch_size=4,  # Decrease this to lower vram usage\n","    num_generations=4,  # Decrease this to lower vram usage\n","    save_strategy=\"no\",  # Do not save checkpoints (saves storage space)\n","    bf16=True,  # Enable bf16 mixed precision on A100 GPUs\n",")\n","\n","trainer = GRPOTrainer(\n","    model=\"microsoft/Phi-4-mini-instruct\",\n","    reward_funcs=reward,\n","    args=training_args,\n","    train_dataset=dataset2,\n",")\n","\n","# Train and save the final model\n","trainer.train()\n","trainer.save_model(os.path.join(checkpoint_dir, \"final_model\"))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["-Raw2ptFPaNR"],"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"main","language":"python","name":"main"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"}},"nbformat":4,"nbformat_minor":0}